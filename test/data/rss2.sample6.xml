<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/rss2full.xsl"?>
<?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0">
  <channel>
    <title><![CDATA[Coding Horror]]></title>
    <description><![CDATA[programming and human factors]]></description>
    <link>https://blog.codinghorror.com/</link>
    <generator>Ghost 0.10</generator>
    <lastBuildDate>Mon, 05 Sep 2016 20:44:04 GMT</lastBuildDate>
    <ttl>60</ttl>
    <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/rss+xml" href="http://feeds.feedburner.com/codinghorror"/>
    <feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="codinghorror"/>
    <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/"/>
    <xhtml:meta xmlns:xhtml="http://www.w3.org/1999/xhtml" name="robots" content="noindex"/>
    <feedburner:emailServiceId xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">codinghorror</feedburner:emailServiceId>
    <feedburner:feedburnerHostname xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">https://feedburner.google.com</feedburner:feedburnerHostname>
    <feedburner:feedFlare xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" href="https://add.my.yahoo.com/rss?url=http%3A%2F%2Ffeeds.feedburner.com%2Fcodinghorror" src="http://us.i1.yimg.com/us.yimg.com/i/us/my/addtomyyahoo4.gif">Subscribe with My Yahoo!</feedburner:feedFlare>
    <feedburner:feedFlare xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" href="http://feeds.my.aol.com/add.jsp?url=http%3A%2F%2Ffeeds.feedburner.com%2Fcodinghorror" src="http://o.aolcdn.com/favorites.my.aol.com/webmaster/ffclient/webroot/locale/en-US/images/myAOLButtonSmall.gif">Subscribe with My AOL</feedburner:feedFlare>
    <feedburner:feedFlare xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" href="http://www.bloglines.com/sub/http://feeds.feedburner.com/codinghorror" src="http://www.bloglines.com/images/sub_modern11.gif">Subscribe with Bloglines</feedburner:feedFlare>
    <feedburner:feedFlare xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" href="http://fusion.google.com/add?feedurl=http%3A%2F%2Ffeeds.feedburner.com%2Fcodinghorror" src="http://buttons.googlesyndication.com/fusion/add.gif">Subscribe with Google</feedburner:feedFlare>
    <feedburner:feedFlare xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" href="http://www.live.com/?add=http%3A%2F%2Ffeeds.feedburner.com%2Fcodinghorror" src="http://tkfiles.storage.msn.com/x1piYkpqHC_35nIp1gLE68-wvzLZO8iXl_JMledmJQXP-XTBOLfmQv4zhj4MhcWEJh_GtoBIiAl1Mjh-ndp9k47If7hTaFno0mxW9_i3p_5qQw">Subscribe with Live.com</feedburner:feedFlare>
    <item>
      <title><![CDATA[Can Software Make You Less Racist?]]></title>
      <description><![CDATA[<p>I don't think we computer geeks appreciate how profoundly the rise of the smartphone, and Facebook, has changed the Internet audience. It's something that really only happened in <strong>the last five years</strong>, as smartphones and data plans dropped radically in price and became accessible &ndash; and addictive &ndash; to huge</p>]]></description>
      <link>https://blog.codinghorror.com/can-software-make-you-less-racist/</link>
      <guid isPermaLink="false">76729083-688d-4f6f-b701-44328f63e63d</guid>
      <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
      <pubDate>Thu, 25 Aug 2016 07:52:37 GMT</pubDate>
      <content:encoded><![CDATA[<p>I don't think we computer geeks appreciate how profoundly the rise of the smartphone, and Facebook, has changed the Internet audience. It's something that really only happened in <strong>the last five years</strong>, as smartphones and data plans dropped radically in price and became accessible &ndash; and addictive &ndash; to huge segments of the population.</p>

      <p>People may have regularly <em>used</em> computers in 2007, sure, but that is a very different thing than having your computer in your pocket, 24/7, with you every step of every day, fully integrated into your life. As Jerry Seinfeld noted in 2014:</p>

      <iframe width="560" height="315" src="https://www.youtube.com/embed/xR1ckgXN8G0" frameborder="0" allowfullscreen></iframe>

      <blockquote>
      <p>But I know you got your phone. Everybody here's got their phone. There's not one person here who doesn't have it. You better have it &hellip; you gotta have it. Because there is no <em>safety</em>, there is no <em>comfort</em>, there is no <em>security</em> for you in this life any more &hellip; unless when you're walking down the street you can feel a <em>hard rectangle in your pants</em>.</p>
      </blockquote>

      <p>It's an addiction that is new to millions &ndash; but eerily familiar to us.</p>

      <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">From &quot;only nerds will use the Internet&quot; to &quot;everyone stares at their smartphones all day long!&quot; in 20 years. Not bad, team :-).</p>&mdash; Marc Andreessen (@pmarca) <a href="https://twitter.com/pmarca/status/556161530455023617">January 16, 2015</a></blockquote>  

      <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

      <p>The good news is that, at this moment, every human being is far more connected to their fellow humans than any human has ever been in the entirety of recorded history.</p>

      <p>Spoiler alert: that's also the bad news. </p>

      <p><a href="https://nextdoor.com/">Nextdoor</a> is a Facebook-alike focused on specific neighborhoods. The idea is that you and everyone else on your block would join, and you can privately discuss local events, block parties, and generally hang out like neighbors do. It's a good idea, and my wife started using it a fair amount in the last few years. We feel more connected to our neighbors through the service. But one unfortunate thing you'll find out when using Nextdoor is that <strong>your neighbors are probably a <em>little bit racist</em>.</strong></p>

      <p>I don't use Nextdoor myself, but I remember Betsy specifically complaining about the casual racism she saw there, and I've also seen it mentioned several times on Twitter by people I follow. They're <a href="http://www.eastbayexpress.com/oakland/racial-profiling-via-nextdoorcom/Content?oid=4526919">not the only ones</a>. It became so epidemic that Nextdoor got a reputation for being a <a href="http://fusion.net/story/106341/nextdoor-the-social-network-for-neighbors-is-becoming-a-home-for-racial-profiling/">racial profiling hub</a>. Which is obviously not good.</p>

      <p>Social networking historically trends young, with the early adopters. Facebook launched as a site for college students. But as those networks grow, they inevitably age. They begin to include older people. And those older people will, statistically speaking, be more racist. I apologize if this sounds ageist, but let me ask you something: do you consider your parents a little racist? I will personally admit that <strong>one of my parents is definitely someone I would label a little bit racist</strong>. It's &hellip; not awesome.  </p>

      <p><a href="http://theoatmeal.com/comics/gay_marriage"><img src="https://blog.codinghorror.com/content/images/2016/08/gay_marriage.png" alt="" title=""></a></p>

      <p>The older the person, the more likely they are to have these "old fashioned" notions that the mere presence of differently-colored people on your block is inherently suspicious, and marriage should <a href="http://en.wikipedia.org/wiki/Public_opinion_of_same-sex_marriage_in_the_United_States#By_age">probably be defined as between a man and a woman</a>.  </p>

      <blockquote>
      <p>In one meta-analysis by Jeffrey Lax and Justin Phillips of Columbia University, a majority of 18–29 year old Americans in 38 states support same sex marriage while in only 6 states do less than 45% of 18–29 year olds support same-sex marriage. At the same time not a single state shows support for same-sex marriage greater than 35% amongst those 64 and older</p>
      </blockquote>

      <p>The idea that regressive social opinions correlate with age isn't an opinion; it's <a href="http://en.wikipedia.org/wiki/Public_opinion_of_same-sex_marriage_in_the_United_States#By_age">a statistical fact</a>.  </p>

      <blockquote>
      <p>Support for same-sex marriage in the U.S.</p>

      <pre><code>18 - 29 years old    65%
      30 - 49 years old    54%
      50 - 64 years old    45%
      65+ years old        39%
      </code></pre>
      </blockquote>

      <p>Are there progressive septuagenarians? Sure there are. But not many.</p>

      <p>To me, failure to support same-sex marriage is as inconceivable as failing to support interracial marriage. Which was <em>not</em> that long ago, to the tune of the late 60s and early 70s. If you want some truly hair-raising reading, <a href="http://en.wikipedia.org/wiki/Loving_v._Virginia">try Loving v. Virginia on for size</a>. Because Virginia <a href="http://www.virginia.org/virginiaisforlovers/">is for lovers</a>. Just not those kind of lovers, 49 years ago. In the interests of full disclosure, I am 45 years old, and I graduated from the University of Virginia.</p>

      <p>With Nextdoor, you're more connected with your neighbors than ever before. But through that connection you may also find out some regressive things about your neighbors that you'd never have discovered in years of the traditional daily routine of polite waves, hellos from the driveway, and casual sidewalk conversations. </p>

      <p>To their immense credit, rather than accepting this status quo, Nextdoor did what any self-respecting computer geek would do: they <strong>changed their software</strong>. Now, when you attempt to post about a crime or suspicious activity &hellip;</p>

      <p><img src="https://blog.codinghorror.com/content/images/2016/08/nextdoor-race-1.png" alt=""></p>

      <p><img src="https://blog.codinghorror.com/content/images/2016/08/nextdoor-race-2.png" alt=""></p>

      <p>&hellip; you get <a href="https://blog.codinghorror.com/the-just-in-time-theory/">smart, just in time nudges</a> to think less about race, and more about <em>behavior</em>.</p>

      <p>The results were <a href="https://www.buzzfeed.com/carolineodonovan/nextdoor-rolls-out-product-fix-it-hopes-will-stem-racial-pro">striking</a>:</p>

      <blockquote>
      <p>Nextdoor claims this new multi-step system has, so far, reduced instances of racial profiling by 75%. It’s also decreased considerably the number of notes about crime and safety. During testing, the number of crime and safety issue reports abandoned before being published rose by 50%. “It’s a fairly significant dropoff,” said Tolia, “but we believe that, for Nextdoor, quality is more important than quantity.”</p>
      </blockquote>

      <p>I'm a huge fan of designing software to help nudge people, at <em>exactly the right time</em>, to <a href="https://blog.codinghorror.com/what-if-we-could-weaponize-empathy/">be their better selves</a>. And this is a textbook example of doing it right.</p>

      <p>Would using Nextdoor and encountering these dialogs make my aforementioned parent a little bit less racist? Probably not. But I like to think they would stop for at least a moment and <strong>consider the importance of focusing on the <em>behavior</em> that is problematic, rather than the individual person</strong>. This is a philosophy I promoted on Stack Overflow, I continue to promote with Discourse, and I reinforce daily with our three kids. You never, ever judge someone by what they look like. Look at <em>what they do</em> instead. </p>

      <p>If you were getting excited about the prospect of validating <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">Betteridge's Law</a> yet again, I'm sorry to disappoint you. I truly do believe software, properly designed software, can not only help us <a href="https://blog.codinghorror.com/civilized-discourse-construction-kit/">be more civil to each other</a>, but can also help people &ndash; maybe even people you love &ndash; <em>behave</em> a bit less like racists online. </p>

      <table>  
      <tr><td class="welovecodinghorror">  
      [advertisement] At Stack Overflow, we help developers learn, share, and grow. Whether you’re looking for your next dream job or looking to build out your team, <a href="http://careers.stackoverflow.com" rel="nofollow">we've got your back</a>.
      </td></tr>  
      </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[The Raspberry Pi Has Revolutionized Emulation]]></title>
  <description><![CDATA[<p>Every geek goes through a phase where they discover emulation. It's practically a <a href="https://blog.codinghorror.com/rediscovering-arcade-nostalgia/">rite of passage</a>.</p>

  <blockquote>
  <p>I think I spent most of my childhood &ndash; and a large part of my life as a young adult &ndash; <b>desperately wishing I was in a video game arcade.</b> When I finally obtained</p></blockquote>]]></description>
  <link>https://blog.codinghorror.com/the-raspberry-pi-has-revolutionized-emulation/</link>
  <guid isPermaLink="false">3d34ef60-a2a4-46e7-b134-236ca0653afb</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Sun, 24 Jul 2016 22:12:40 GMT</pubDate>
  <content:encoded><![CDATA[<p>Every geek goes through a phase where they discover emulation. It's practically a <a href="https://blog.codinghorror.com/rediscovering-arcade-nostalgia/">rite of passage</a>.</p>

  <blockquote>
  <p>I think I spent most of my childhood &ndash; and a large part of my life as a young adult &ndash; <b>desperately wishing I was in a video game arcade.</b> When I finally obtained my driver's license, my first thought wasn't about the girls I would take on dates, or the road trips I'd take with my friends. Sadly, no. I was thrilled that I could drive myself to the arcade any time I wanted. </p>
  </blockquote>

  <p>My two arcade emulator builds in 2005 satisfied my itch thoroughly. I recently took my son Henry to the <a href="http://www.caextreme.org/">California Extreme expo</a>, which features almost every significant pinball and arcade game ever made, live and in person and real. He enjoyed it so much that I found myself again yearning to share that part of our history with my kids &ndash; in a suitably emulated, arcade form factor.</p>

  <p>Down, down the rabbit hole I went again:</p>

  <p><a href="http://www.hybridarcades.com/"><img src="https://blog.codinghorror.com/content/images/2016/08/bartop-ebay.jpg" alt="" title=""></a></p>

  <p><a href="https://www.arcademinis.com/sunshop/index.php?l=product_detail&amp;p=60"><img src="https://blog.codinghorror.com/content/images/2016/07/bartop-vertical-jamma.jpg" alt="" title=""></a></p>

  <p><a href="https://shop.pimoroni.com/collections/picade-and-arcade/products/picade"><img src="https://blog.codinghorror.com/content/images/2016/07/picade.jpg" alt="" title=""></a></p>

  <p><a href="http://forum.arcadecontrols.com/index.php?topic=122366.0"><img src="https://blog.codinghorror.com/content/images/2016/07/bartop-bubble-bobble.jpg" alt="" title=""></a></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/07/bartop-cocktail.jpg" alt=""></p>

  <p>I discovered that emulation builds are so much <em>cheaper and easier</em> now than they were when I last attempted this a decade ago. Here's why:</p>

  <ol>
  <li><p><strong>The ascendance of <a href="https://www.raspberrypi.org/">Raspberry Pi</a> has single-handedly revolutionized the emulation scene.</strong> The Pi is now on <a href="https://www.raspberrypi.org/products/raspberry-pi-3-model-b/">version 3</a>, which adds critical WiFi and Bluetooth functionality on top of additional speed. It's fast enough to emulate N64 and PSX and Dreamcast reasonably, all for a whopping $35. Just download the <a href="https://retropie.org.uk/">RetroPie bootable OS</a> on a $10 32GB SD card, slot it into your Pi, and &hellip; well, basically you're done. The distribution comes with some free games on it. Add additional ROMs and game images to taste.</p></li>
  <li><p><strong>Chinese all-in-one JAMMA cards</strong> are available everywhere for about $90. <a href="https://www.youtube.com/watch?v=ctN60cYTRdI">Pandora's Box is one "brand"</a>. These things are are an entire 60-in-1 to 600-in-1 arcade on a board, with an ARM CPU and built-in ROMs and everything &hellip; probably completely illegal and unlicensed, of course. You could buy some old broken down husk of an arcade game cabinet, anything at all as long as it's a <a href="https://en.wikipedia.org/wiki/Japan_Amusement_Machine_and_Marketing_Association#Connector_standards">JAMMA compatible arcade game</a> &ndash; a standard introduced in 1985 &ndash; with working monitor and controls. Plug this replacement JAMMA box in, and bam: you now have your own virtual arcade. Or you could build or buy a new JAMMA compatible cabinet; there are hundreds out there to choose from.</p></li>
  <li><p><strong>Cheap, quality IPS arcade size LCDs</strong>. The CRTs I used in 2005 may have been truer to old arcade games, but they were a giant pain to work with. They're enormous, heavy, and require a lot of power. Viewing angle and speed of refresh are rather critical for arcade machines, and both are largely solved problems for LCDs at this point, which are light, easy to work with, and sip power for $100 or less.</p></li>
  </ol>

  <p>Add all that up &ndash; it's not like the price of MDF or arcade buttons and joysticks has changed substantially in the last decade &ndash; and what we have today is a console and arcade emulation wonderland! If you'd like to go down this rabbit hole with me, bear in mind that I've just started, but I do have some specific recommendations.</p>

  <p><strong>Get a Raspberry Pi starter kit.</strong> I recommend <a href="http://www.amazon.com/dp/B01D92SSX6/?tag=codihorr-20">this particular starter kit</a>, which includes the essentials: a clear case, heatsinks &ndash; you definitely want small heatsinks on your 3, as it dissipate <a href="http://www.pidramble.com/wiki/benchmarks/power-consumption">almost 4 watts</a> under full load &ndash; and a suitable power adapter. That's $50.</p>

  <p><a href="http://www.amazon.com/dp/B01D92SSX6/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/07/raspberry-pi-port-diagram.jpg" alt="" title=""></a></p>

  <p><strong>Get a quality SD card.</strong> The primary "drive" on your Pi will be the SD card, so make it a quality one. Based on <a href="http://www.pidramble.com/wiki/benchmarks/microsd-cards">these excellent benchmarks</a>, I recommend the <a href="http://www.amazon.com/dp/B013CP5HCK/?tag=codihorr-20">Sandisk Extreme 32GB</a> or <a href="http://www.amazon.com/dp/B00WR4IJBE/?tag=codihorr-20">Samsung Evo+ 32GB</a> models for best price to peformance ratio. That'll be $15, tops.</p>

  <p><strong>Download and install the bootable RetroPie image on your SD card.</strong> It's amazing how far this project has come since 2013, it is now about as close to plug and play as it gets for free, open source software. The install is, <a href="https://github.com/retropie/retropie-setup/wiki/First-Installation">dare I say &hellip; "easy"?</a></p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/xvYX_7iRRI0" frameborder="0" allowfullscreen></iframe>

  <p><strong>Decide how much you want to build.</strong> At this point you have a fully functioning emulation brain for well under $100 which is capable of playing literally <em>every significant console and arcade game created prior to 1997</em>. Your 1985 self is probably drunk with power. It is kinda awesome. Stop doing the Safety Dance for a moment and ask yourself these questions:</p>

  <ul>
  <li><p>What <strong>controls</strong> do you plan to plug in via the USB ports? This will depend heavily on which games you want to play. Beyond the absolute basics of joystick and two buttons, there are Nintendo 64 games (think analog stick(s) required), driving games, spinner and trackball games, multiplayer games, yoke control games (think Star Wars), virtual gun games, and so on.</p></li>
  <li><p>What <strong>display</strong> to you plan to plug in via the HDMI port? You could go with a tiny screen and build a handheld emulator, the Pi is certainly small enough. Or you could have no display at all, and jack in via HDMI to any nearby display for whatever gaming jamboree might befall you and your friends. I will say that, for whatever size you build, <em>more display is better</em>. Absolutely go as big as you can in the allowed form factor, though the Pi won't effectively use much more than a 1080p display maximum.</p></li>
  <li><p>How much <strong>space</strong> do you want to dedicate to the box? Will it be portable? You could go anywhere from ultra-minimalist &ndash; a control box you can plug into any HDMI screen with a wireless controller &ndash; to a giant 40" widescreen stand up arcade machine with room for four players.</p></li>
  <li><p>What's your <strong>budget</strong>? We've only spent under $100 at this point, and great screens and new controllers aren't a whole lot more, but sometimes you want to build from spare parts you have lying around, if you can.</p></li>
  <li><p>Do you have the <strong>time</strong> and inclination to build this from parts? Or do you prefer to buy it pre-built?</p></li>
  </ul>

  <p>These are all your calls to make. You can get some ideas from the pictures I posted at the top of this blog post, or search the web for "Raspberry Pi Arcade" for <a href="http://www.slothygeek.com/6-raspberry-pi-arcade-projects-step-by-step-tutorials/">lots of other ideas</a>.</p>

  <p>As a reasonable all-purpose starting point, I recommend <a href="http://www.retrobuiltgames.com/diy-kits-shop/porta-pi-arcade-wood-kit-10-hd/">the Build-Your-Own-Arcade kits</a> from Retro Built Games. From $330 for full kit, to $90 for just the wood case.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/07/retrobuiltgames-diy-pi-arcade.jpg" alt=""></p>

  <p>You could also buy <a href="http://www.amazon.com/dp/B00WAY9848/?tag=codihorr-20">the arcade controls alone</a> for $75, and build out (or buy) a case to put them in.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/07/arcade-controls-for-pi.jpg" alt=""></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/07/arcade-controls-mdf-case.jpg" alt=""></p>

  <p>My "mainstream" recommendation is <strong>a bartop arcade</strong>. It uses a common LCD panel size in the typical horizontal orientation, it's reasonably space efficient and somewhat portable, while still being comfortably large enough for a nice big screen with large speakers gameplay experience, and it supports two players if that's what you want. That'll be about $100 to $300 depending on options.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/07/bartop-arcade-kit.jpg" alt=""></p>

  <p>I remember spending well over $1,500 to build <a href="https://blog.codinghorror.com/rediscovering-arcade-nostalgia/">my old arcade cabinets</a>. I'm excited that it's no longer necessary to invest that much time, effort or money to successfully revisit our arcade past.</p>

  <p>Thanks largely to the <a href="http://www.amazon.com/dp/B01D92SSX6/?tag=codihorr-20">Raspberry Pi 3</a> and the <a href="https://retropie.org.uk/">RetroPie project</a>, this is now a simple Maker project you can (and should!) take on in a weekend with a friend or family. For a budget of $100 to $300 &ndash; maybe $500 if you want to get <em>extra</em> fancy &ndash; you can have a pretty great classic arcade and classic console emulation experience. That's way better than I was doing in 2005, even adjusting for inflation.</p>

  <table>  
  <tr><td class="welovecodinghorror">[advertisement] At Stack Overflow, we put developers first. We already help you find answers to your tough coding questions; now let us help you <a href="http://careers.stackoverflow.com" rel="nofollow">find your next job</a>.</td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[The Golden Age of x86 Gaming]]></title>
  <description><![CDATA[<p>I've been happy with <a href="https://blog.codinghorror.com/the-2016-htpc-build/">my 2016 HTPC</a>, but the situation has changed, largely because of something I mentioned in passing back in November:</p>

  <blockquote>
  <p>The Xbox One and PS4 are <a href="http://www.extremetech.com/gaming/156273-xbox-720-vs-ps4-vs-pc-how-the-hardware-specs-compare">effectively plain old PCs</a>, built on:</p>
  
  <ul>
  <li>Intel Atom class (aka slow) AMD 8-core x86 CPU</li>
  <li>8 GB RAM</li>
  <li>AMD Radeon 77xx</li></ul></blockquote>]]></description>
  <link>https://blog.codinghorror.com/the-golden-age-of-x86-gaming/</link>
  <guid isPermaLink="false">853472d9-e870-4a63-b54d-3bc417830a12</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 20 May 2016 22:05:59 GMT</pubDate>
  <content:encoded><![CDATA[<p>I've been happy with <a href="https://blog.codinghorror.com/the-2016-htpc-build/">my 2016 HTPC</a>, but the situation has changed, largely because of something I mentioned in passing back in November:</p>

  <blockquote>
  <p>The Xbox One and PS4 are <a href="http://www.extremetech.com/gaming/156273-xbox-720-vs-ps4-vs-pc-how-the-hardware-specs-compare">effectively plain old PCs</a>, built on:</p>
  
  <ul>
  <li>Intel Atom class (aka slow) AMD 8-core x86 CPU</li>
  <li>8 GB RAM</li>
  <li>AMD Radeon 77xx / 78xx GPUs</li>
  <li>cheap commodity 512GB or 1TB hard drives (not SSDs)</li>
  </ul>
  
  <p>The <strong>golden age of x86 gaming</strong> is well upon us. That's why the future of PC gaming is looking brighter every day. We can see it coming true in the solid GPU and idle power improvements in Skylake, riding the inevitable wave of x86 becoming the dominant kind of (non mobile, anyway) gaming for the forseeable future.</p>
  </blockquote>

  <p>And then, the bombshell. It is <a href="http://arstechnica.com/gaming/2016/04/ps4k-neo-details-specs-revealed-rumours/">all but announced</a> that Sony will be upgrading the PS4 this year, no more than three years after it was first introduced &hellip; <strong>just like you would upgrade a PC.</strong></p>

  <blockquote>
  <p>Sony may be tight-lipped for now, but it's looking increasingly likely that the company will release an updated version of the PlayStation 4 later this year. So far, the rumoured console has gone under the moniker PS4K or PS4.5, but a new report from gaming site GiantBomb suggests that the codename for the console is "NEO," and it even provides hardware specs for the PlayStation 4's improved CPU, GPU, and higher bandwidth memory.</p>
  
  <ul>
  <li>CPU: 1.6 &rarr; 2.1 Ghz CPU </li>
  <li>GPU: 18 CUs @ 800Mhz &rarr; 36 CUs @ 911Mhz</li>
  <li>RAM: 8GB DDR5 176 GB/s &rarr; 218 GB/s</li>
  </ul>
  </blockquote>

  <p>In PC enthusiast parlance, you might say Sony just slotted in a new video card, a faster CPU, and slightly higher speed RAM.</p>

  <p><a href="http://acommonblog.com/2011/09/the-golden-age-logo-design/"><img src="https://blog.codinghorror.com/content/images/2016/05/goldenage-logo.jpg" alt="" title=""></a></p>

  <p>This is old hat for PCs, but to release a new, faster model that is perfectly backwards compatible is almost unprecedented in the console world. I have to wonder if this is partially due to <a href="https://blog.codinghorror.com/i-tried-vr-and-it-was-just-ok/">the intense performance pressure of VR</a>, but whatever the reason, I applaud Sony for taking this step. It's a giant leap towards consoles being more like PCs, and <strong>another sign that the golden age of x86 is really and truly here.</strong></p>

  <p>I hate to break this to PS4 enthusiasts, but as big of an upgrade as that is &ndash; and it really is &ndash; it's still nowhere <em>near</em> enough power to drive modern games at 4k. Nvidia's <a href="http://arstechnica.com/gadgets/2016/05/nvidia-gtx-1080-review/">latest and greatest 1080 GTX</a> can only sometimes manage 30fps at 4k. The increase in required GPU power when going from 1080p to 4k is so vast that even the PC "cost is no object" folks who will happily pay $600 for a video card and $1000 for the rest of their box have some difficulty getting there today. Stuffing all that into a $299 box for the masses is going to take quite a few more years.</p>

  <p><a href="https://blog.codinghorror.com/our-brave-new-world-of-4k-displays/"><img src="https://blog.codinghorror.com/content/images/2016/05/4k-vs-1080p-1.png" alt="" title=""></a></p>

  <p>Still, I like the idea of the PS4 Neo so much that I'm considering buying it myself.  I strongly support this sea change in console upgradeability, even though I swore I'd stick with the Xbox One this generation. To be honest, my Xbox One has been a disappointment to me. I bought the "Elite" edition because it had a hybrid 1TB drive, and then added a 512GB USB 3.0 SSD to the thing and painstakingly moved all my games over to that, and it is <em>still</em> appallingly slow to boot, to log in, to page through the UI, to load games. It's also noisy under load and sounds like a broken down air conditioner even when in low power, background mode. The Xbox One experience is way too often drudgery and random errors instead of the gaming fun it's supposed to be. Although I do unabashedly love the new controller, I feel like the Xbox One is, overall, a worse gaming experience than the Xbox 360 was. And that's sad.</p>

  <p>Or maybe I'm just spoiled by PC performance, and the relatively crippled flavor of PC you get in these $399 console boxes. If all evidence points to the golden age of x86 being upon us, why not double down on x86 in the living room? Heck, while I'm at it &hellip; why not <em>triple down?</em></p>

  <p><a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/05/skull-canyon-nuc-case-top.jpg" alt="" title=""></a></p>

  <p>This, my friends, is what tripling down on x86 in the living room looks like.</p>

  <p>It's <a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20">Intel's latest Skull Canyon NUC</a>. What does that acronym stand for? Too embarrassing to explain. Let's just pretend it means "tiny awesome x86 PC". What's significant about this box is it contains <strong>the first on-die GPU Intel has ever shipped that can legitimately be considered console class</strong>.</p>

  <p><a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/05/skull-canyon-nuc-front-and-back-ports-1.jpg" alt="" title=""></a></p>

  <p>It's <a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20">not cheap at $579</a>, but this tiny box bristles with cutting edge x86 tech:</p>

  <ul>
  <li>Quad-core i7-6770HQ CPU (2.6 Ghz / 3.5 Ghz)</li>
  <li>Iris Pro Graphics 580 GPU with 128MB eDRAM</li>
  <li>Up to 32GB DDR4-2666 RAM</li>
  <li>Dual M.2 PCI x4 SSD slots</li>
  <li>802.11ac WiFi / Bluetooth / Gigabit Ethernet</li>
  <li>Thunderbolt 3 / USB 3.1 gen 2 Type-C port</li>
  <li>Four USB 3.0 ports</li>
  <li>HDMI 2.0, mini-DP 1.2 video out</li>
  <li>SDXC (UHS-I) card reader</li>
  <li>Infrared sensor</li>
  <li>3.5mm combo digital / optical out port</li>
  <li>3.5mm headphone jack</li>
  </ul>

  <p>All impressive, but the most remarkable items are the GPU and the Thunderbolt 3 port. Putting together a HTPC that can kick an Xbox One's butt as a gaming box is now as simple as adding these three items together:</p>

  <ol>
  <li><a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20">Intel NUC kit NUC6i7KYK</a> $579  </li>
  <li><a href="http://www.amazon.com/dp/B014R8JWEA/?tag=codihorr-20">16GB DDR4-2400</a> $75  </li>
  <li><a href="http://www.amazon.com/dp/B01639694M/?tag=codihorr-20">Samsung 950 Pro NVMe M.2 (512GB)</a> $317</li>
  </ol>

  <p>Ok, fine, it's a cool <strong>$970</strong> plus tax compared to $399 for one of those console x86 boxes. But did I mention it has <em>skulls</em> on it? <em>Skulls!</em></p>

  <p>The CPU and disk performance on offer here are hilariously far beyond what's available on current consoles: </p>

  <ul>
  <li><p>Disk performance of the two internal PCIe 3.0 4x M.2 slots, assuming you choose a proper NVMe drive as you should, is measured in not megabytes per second but <a href="http://www.anandtech.com/show/10303/choosing-the-right-ssd-for-a-skylakeu-system/2"><em>gigabytes</em> per second.</a> Meanwhile consoles lumber on with, at best, hybrid drives.</p></li>
  <li><p>The Jaguar class AMD x86 cores in the Xbox One and PS4 are about the same as the <a href="http://anandtech.com/show/7314/intel-baytrail-preview-intel-atom-z3770-tested/2">AMD A4-5000 reviewed here</a>; those benchmarks indicate a modern Core i7 will be about <a href="http://www.anandtech.com/show/7003/the-haswell-review-intel-core-i74770k-i54560k-tested/6">four times faster</a>.</p></li>
  </ul>

  <p>But most importantly, <a href="http://nucblog.net/2016/05/skull-canyon-nuc-review-conclusion/">its GPU performance is on par with current consoles</a>. NUC blog measured <strong>41fps average</strong> in Battlefield 4 at 1080p and medium settings. Digging through old benchmarks I find plenty of pages where a Radeon 78xx or 77xx series video card, the closest analog to what's in the XBox One and PS4, achieves a <a href="http://www.bit-tech.net/hardware/graphics/2013/11/27/battlefield-4-performance-analysis/3">similar result in Battlefield 4</a>:</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/radeon-77xx-perf-bf4-1080p-medium.png" alt=""></p>

  <p>I personally benchmarked GRID 2 at 720p (high detail) on all three of the last HTPC models I owned:</p>

  <table width="320px">  
  <tr>  
  <td></td><td>Max</td><td>Min</td><td>Avg  
  </td></tr>  
  <tr>  
  <td>i3-4130T, HD 4400</td><td>32</td><td>21</td><td>27  
  </td></tr>  
  <tr>  
  <td>i3-6100T, HD 530</td><td>50</td><td>32</td><td>39  
  </td></tr>  
  <tr>  
  <td>i7-6770HQ, Iris Pro 580</td><td>96</td><td>59</td><td>78  
  </td></tr>  
  </table>

  <p>When I up the resolution to 1080p, I get <strong>59fps average</strong>, 38 min, 71 max. Checking with <a href="http://www.notebookcheck.net/Computer-Games-on-Laptop-Graphics-Cards.13849.0.html">Notebookcheck's exhaustive benchmark database</a>, that is closest to the AMD R7 250, a rebranded Radeon 7770.</p>

  <p>What we have here is legitimately the first on-die GPU that can compete with a low-end discrete video card from AMD or Nvidia. Granted, an older one, one you could buy for about $80 today, but one that is certainly equivalent to what's in the Xbox One and PS4 <em>right now</em>. This is a real first for Intel, and it probably won't be the last time, considering that on-die GPU performance increases have massively outpaced CPU performance increases for the last 5 years.</p>

  <p>As for power usage, I was pleasantly surprised to measure that this box idles at <b>15w</b> at the Windows Desktop doing nothing, and drops to <b>13w</b> when the display sleeps. Considering the best idle numbers I've measured are from the <a href="https://blog.codinghorror.com/the-scooter-computer/">Scooter Computer at 7w</a> and <a href="https://blog.codinghorror.com/the-2016-htpc-build/">my previous HTPC build at 10w</a>, that's not bad at all! Under full game load, it's more like 70 to 80 watts, and in typical light use, 20 to 30 watts. It's the idle number that matters the most, as that represents the typical state of the box. And compared to <a href="http://www.extremetech.com/gaming/182829-new-report-slams-xbox-one-and-ps4-power-consumption-inefficiencies-still-abound">the 75 watts a console uses even when idling at the dashboard</a>, it's no contest.</p>

  <p>Of course, 4k video playback is no problem, though 10-bit 4K video <a href="http://jell.yfish.us/">may be a stretch</a>. If that's not enough &mdash; if you dream bigger than medium detail 1080p gameplay &mdash; the presence of a Thunderbolt 3 port on this little box means you can, at considerable expense, use <strong>any external GPU of your choice</strong>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/razer-core-external-GPU.jpg" alt=""></p>

  <p>That's the <a href="http://www.razerzone.com/store/razer-core">Razer Core external graphics dock</a>, and it's $499 all by itself, but it opens up an entire world of upgrading your GPU to whatever the heck you want, as long as your x86 computer has a Thunderbolt 3 port. And it really works! In fact, here's a video of it working live with this exact configuration:</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/BUxP1Uwp7so" frameborder="0" allowfullscreen></iframe>

  <p>Zero games are meaningfully CPU limited today; the disk and CPU performance of this Skull Canyon NUC is already so vastly far ahead of current x86 consoles, even the PS4 Neo that's about to be introduced. So being able to replace the one piece that needs to be the most replaceable is huge. Down the road you can add the latest, greatest GPU model whenever you want, just by plugging it in!</p>

  <p>The only downside of using such a small box as my HTPC is that my two 2.5" 2TB media drives become external USB 3.0 enclosures, and I am limited by the 4 USB ports. So it's a little &hellip; cable-y in there. But I've come to terms with that, and its tiny size is an acceptable tradeoff for all the cable and dongle overhead.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/skull-canyon-with-razer-core-external.jpg" alt=""></p>

  <p>I still remember how shocked I was <a href="https://blog.codinghorror.com/x86-uber-alles/">when Apple switched to x86 back in 2005</a>. I was also surprised to discover just how thoroughly both the PS4 and Xbox One embraced x86 in 2013. Add in the current furor over VR, plus the PS4 Neo opening new console upgrade paths, and the future of x86 as a gaming platform is rapidly approaching supernova.</p>

  <p>If you want to experience what console gaming will be like in 10 years, invest in a <a href="http://www.amazon.com/dp/B01DJ9XS52/?tag=codihorr-20">Skull Canyon NUC</a> and an external Thunderbolt 3 graphics dock today. <strong>If we are in a golden age of x86 gaming, this configuration is its logical endpoint.</strong></p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] <a href="http://careers.stackoverflow.com" rel="nofollow">Find a better job the Stack Overflow way</a> - what you need when you need it, no spam, and no scams.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Your Own Personal WiFi Storage]]></title>
  <description><![CDATA[<p>Our kids have reached the age &ndash; at ages 4, 4, and 7 respectively &ndash; that taking longer trips with them is now possible without everyone losing what's left of their sanity in the process. But we still have the same problem on multiple hour trips, whether it's in a</p>]]></description>
  <link>https://blog.codinghorror.com/your-own-personal-wifi-storage/</link>
  <guid isPermaLink="false">708fb957-56e2-48d9-ae55-a8bbbc883fcc</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Sat, 07 May 2016 06:42:23 GMT</pubDate>
  <content:encoded><![CDATA[<p>Our kids have reached the age &ndash; at ages 4, 4, and 7 respectively &ndash; that taking longer trips with them is now possible without everyone losing what's left of their sanity in the process. But we still have the same problem on multiple hour trips, whether it's in a car, or on a plane &ndash; how do we bring enough stuff to keep the kids entertained without carting 5 pounds of books and equipment along, per person? And if we agree, like most parents, that <a href="https://twitter.com/jimgaffigan/status/649355348013395968">the iPad is the general answer to this question</a>, how do I get enough local media downloaded and installed on each of their iPads before the trip starts? And do I need 128GB iPads, because those are kind of expensive?</p>

  <p>We clearly have a media sharing problem. I asked on Twitter and quite a number of people recommended the <a href="http://www.amazon.com/dp/B00RVIGY1I/?tag=codihorr-20">HooToo HT-TM05 TripMate Titan</a> at $40. I took their advice, and they were right &ndash; <strong>this little device is amazing!</strong></p>

  <p><a href="http://www.amazon.com/dp/B00RVIGY1I/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/05/HT-TM05.jpg" alt="" title=""></a></p>

  <ul>
  <li>10400mAh External Battery</li>
  <li>WiFi USB 3.0 media sharing device</li>
  <li>Wired-to-WiFi converter</li>
  <li>WiFi-to-WiFi bridge to share a single paid connection </li>
  </ul>

  <p>The value of the last two points is debatable depending on your situation, but the utility of the first two is <em>huge!</em> Plus the large built in battery means it can act as a self-powered WiFi hotspot for 10+ hours. All this for <a href="http://www.amazon.com/dp/B00RVIGY1I/?tag=codihorr-20">only forty bucks!</a></p>

  <p>It's a very simple device. It has exactly <strong>one button</strong> on the top:</p>

  <ul>
  <li>Hold the button down for 5+ seconds to power on or off.</li>
  <li>Tap the button to see the current battery level, represented as 1-4 white LEDs.</li>
  <li>The blue LED will change to green if connected to another WiFi or wired network.</li>
  </ul>

  <p>Once you <a href="http://www.amazon.com/dp/B00RVIGY1I/?tag=codihorr-20">get yours</a>, just hold down the button to power it on, let it fully boot, and connect to the new <code>TripMateSith</code> WiFi network. As to why it's called that, I suspect it has to do with the color scheme of the device and this guy.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/darth-maul.jpg" alt=""></p>

  <p>I am guessing licensing issues forced them to pick the 'real' name of TripMate Titan, but wirelessly, it's known as <code>TripMateSith-XXXX</code>. Connect to that. The default password is <code>11111111</code> (that's eight ones).</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/wifi-browser-hootoo-titan.png" width="320px"></p>

  <p>Once connected, navigate to <code>10.10.10.254</code> in your browser. Username is <code>admin</code>, no password. </p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/hootoo-initial-connect.png" width="320px"></p>

  <p>This interface is totally smartphone compatible, for the record, but I recommend you do this from a desktop or laptop since we need to upgrade the firmware immediately. As received, the device has firmware 2.000.022 and you'll definitely want to <a href="http://www.hootoo.com/downloads-HT-TM05.html">upgrade to the latest firmware</a> right away:</p>

  <ul>
  <li>Make sure a small USB storage device is attached &ndash; it needs local scratch disk space to upgrade.</li>
  <li>You'd think putting the firmware on a USB storage device and inserting said USB storage device into the HooToo would work, and I agree that's logical, but &hellip; you'd be wrong. </li>
  <li>Connect from a laptop or desktop, then visit the Settings, Firmware page and upload the firmware file from there. (I couldn't figure out any way to upgrade firmware from a phone, at least not on iOS.)</li>
  </ul>

  <h4 id="storage">Storage</h4>

  <p>For this particular use, so we can attach the storage, leave it attached forever, and kinda-sorta pretend it is all one device, I recommend a <a href="http://www.amazon.com/dp/B01BGTG2A0/?tag=codihorr-20">tiny $32 128GB USB 3.0 drive</a>. It's not a barn-burner, but it's fast enough for its diminutive size.</p>

  <p><a href="http://www.amazon.com/dp/B01BGTG2A0/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/05/sandisk-fit-128gb.jpg" width="320px"></a></p>

  <p>In the past, <a href="http://blog.codinghorror.com/a-ssd-in-your-pocket/">I've recommended very fast USB 3.0 drives</a>, but I think that time is coming to an end. If you need something larger than 128GB, you could carry a USB 3.0 enclosure with a traditional inexpensive 2.5" HD, but the combination of travel and spinning hard drives makes me nervous. Not to mention the extra power consumption. Instead, I recommend one of the new, budget compact M.2 SSDs in a USB 3.0 enclosure:</p>

  <ul>
  <li><a href="http://www.amazon.com/dp/B01BO4L3MY/?tag=codihorr-20">480GB M.2 2280 SATA SSD</a> ($130)</li>
  <li><a href="http://www.amazon.com/dp/B019NNEA2I/?tag=codihorr-20">M.2 SATA to USB 3.0 Enclosure</a> ($23)</li>
  </ul>

  <p><a href="http://www.amazon.com/dp/B01BO4L3MY/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2016/05/usb-3-sata-m2-enclosure-1.jpg" width="600px"></a></p>

  <p>I discovered this brand of Phison controller based budget M.2 SSDs when I <a href="http://blog.codinghorror.com/the-scooter-computer/">bought the Scooter Computers</a> and they are surprisingly great performers for the price, particularly if you stick with the newest Phison S10 controller. And they run absolute <em>circles</em> around large USB flash drives in performance! The larger the drive, believe me, the more you need to care about this, like say you need to quickly copy a bunch of reasonably new media for the kids to enjoy before you go catch that plane.</p>

  <h4 id="settingsandwifi">Settings and WiFi</h4>

  <p>Let's continue setting up our HooToo Tripmate Titan. In the web interface, under Settings, Network Settings, these are the essentials:</p>

  <ul>
  <li><p>In Host Name, first <strong>set the device name to something short and friendly</strong>. You will be typing this later on every device you attach to it. I used <code>mully</code> and <code>sully</code> for mine.</p></li>
  <li><p>In Wi-Fi and LAN</p>

  <ul><li><p>pick <strong>a strong, long WiFi password</strong>, because there's very little security on the device beyond the WiFi gate.</p></li>
  <li><p>set the <a href="http://www.extremetech.com/computing/179344-how-to-boost-your-wifi-speed-by-choosing-the-right-channel">WiFi channel to either 1, 6, or 11</a> so you are not crowding around other channels.</p></li>
  <li><p>set security to <a href="http://security.stackexchange.com/questions/23627/strength-of-wep-wpa-and-wpa-2-psk">WPA2-PSK only</a>. No need to support old, insecure connection types.</p></li></ul></li>
  </ul>

  <p>There's more here, if you want to bridge wired or wirelessly, but this will get you started.</p>

  <h4 id="windows">Windows</h4>

  <p>Connect to the HooToo's WiFi network, then type in the name of the device (mine's called <code>sully</code>) in Explorer or the File Run dialog, prefixed by <code>\\</code>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/Screenshot--17-.png" width="480px"></p>

  <p>The default user accounts are <code>admin</code> and <code>guest</code> with no passwords, unless you set one up. Admin lets you write files; guest does not.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/Screenshot--18-.png" width="480px"></p>

  <p>Once you connect you'll see the default file share for the USB device and can begin browsing the files at <code>UsbDisk1_Volume1</code>. </p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/Screenshot--19-.png" width="480px"></p>

  <h4 id="ios">iOS</h4>

  <p>I use the <a href="https://itunes.apple.com/us/app/fileexplorer/id499470113?mt=8">File Explorer app</a> for iOS, though I am sure there are plenty of other alternatives. It's $5, and I have it installed on all my iOS devices.</p>

  <p>Connect to the HooToo's WiFi network, then add a new Windows type share via the menu on the left. (I'm not sure if other share types work, they might, but that one definitely does.) Enter the name of the device here and the account <code>admin</code> with no password. If you forget to enter account info, you'll get prompted on connect.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/IMG_0371.PNG" width="480px"></p>

  <p>Once set up, this connection will be automatically saved for future use. And once you connect, you can browse the single available file share at <code>UsbDisk1_Volume1</code> and play back any files.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/05/IMG_0372.PNG" width="640px"></p>

  <p>Be careful, though, as media files you open here will use the default iOS player &ndash; you may need a third party media player if the file has complex audio streams (DTS, for example) or unusual video encoders. </p>

  <h4 id="caveats">Caveats</h4>

  <p>For some reason, with a USB 3.0 flash drive attached, the battery slowly drains even when powered off. So you'll want to remove any flash drive when the HooToo is powered off for extended periods. I have no idea why this happens, but I was definitely able to reproduce the behavior. Kind of annoying since my whole goal was to have "one" device, but oh well.</p>

  <p>This isn't a <a href="http://www.howtogeek.com/252261/how-to-set-up-plex-and-watch-your-movies-on-any-device/">fancy, glitzy Plex based system</a>, it's a basic filesystem browser. Devices that have previously connected to this WiFi network will definitely connect to it when no other WiFi networks are available, like say, when you're in a van driving to Legoland, or on a plane flying to visit your grandparents. You will still have to train people to visit the File Explorer app, and the right device name to look for, or create a desktop link to the proper share.</p>

  <p>But in my book, simple is good. The <a href="http://www.amazon.com/dp/B00RVIGY1I/?tag=codihorr-20">HooToo HT-TM05 TripMate</a> plus a <a href="http://www.amazon.com/dp/B01BGTG2A0/?tag=codihorr-20">small 128GB flash drive</a>  is an easy, flexible way to wirelessly share large media files across a ton of devices for less than 75 bucks total, and it comes with a large, convenient rechargeable battery. </p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/jv_jkju_iZg?start=95" frameborder="0" allowfullscreen></iframe>

  <p>I think one of these will live, with its charger cable and a flash drive chock full of awesome media, permanently inside our van for the kids. Remember, <strong>no matter where you go, there your &hellip; files &hellip; are.</strong></p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] Building out your tech team? <a href="http://careers.stackoverflow.com/products" rel="nofollow">Stack Overflow Careers</a> helps you hire from the largest community for programmers on the planet. We built our site with developers like you in mind.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[They Have To Be Monsters]]></title>
  <description><![CDATA[<p>Since I started working on Discourse, I spend a lot of time thinking about how software can encourage and nudge people to be more empathetic online. That's why it's troubling to read articles <a href="https://medium.com/@stephaniewittelswachs/the-end-of-empathy-5d8383b066d3">like this one</a>:</p>

  <blockquote>
  <p>My brother’s 32nd birthday is today. It’s an especially emotional day for</p></blockquote>]]></description>
  <link>https://blog.codinghorror.com/they-have-to-be-monsters/</link>
  <guid isPermaLink="false">61da2fa6-6508-4e8d-ae45-096add451ce3</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 29 Apr 2016 21:47:53 GMT</pubDate>
  <content:encoded><![CDATA[<p>Since I started working on Discourse, I spend a lot of time thinking about how software can encourage and nudge people to be more empathetic online. That's why it's troubling to read articles <a href="https://medium.com/@stephaniewittelswachs/the-end-of-empathy-5d8383b066d3">like this one</a>:</p>

  <blockquote>
  <p>My brother’s 32nd birthday is today. It’s an especially emotional day for his family because he’s not alive for it.</p>
  
  <p>He died of a heroin overdose last February.
  This year is even harder than the last. I started weeping at midnight and eventually cried myself to sleep. Today’s symptoms include explosions of sporadic sobbing and an insurmountable feeling of emptiness. My mom posted a gut-wrenching comment on my brother’s Facebook page about the unfairness of it all. Her baby should be here, not gone. “Where is the God that is making us all so sad?” she asked.  </p>
  
  <p>In response, someone — a stranger/(I assume) another human being — commented with one word: “Junkie.”</p>
  </blockquote>

  <p>The interaction may seem a bit strange and out of context until you realize that this is the Facebook page of a person who was somewhat famous, who <a href="http://variety.com/2015/tv/news/parks-and-recreation-producer-dead-harris-wittels-1201437460/">produced the excellent show Parks and Recreation</a>. Not that this forgives the behavior in any way, of course, but it does explain why strangers would wander by and make observations.</p>

  <p>There is deep truth in the old idea that people are able to say these things <strong>because they are looking at a screen full of words</strong>, not directly at the face of the person they're about to say a terrible thing to. That one level of abstraction the Internet allows, typing, which is so immensely powerful in so many other contexts &hellip;</p>

  <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">“falling in love, breaking into a bank, bringing down the govt…they all look the same right now: they look like typing” <a href="https://twitter.com/PennyRed">@PennyRed</a> <a href="https://twitter.com/hashtag/TtW16?src=hash">#TtW16</a> <a href="https://twitter.com/hashtag/k3?src=hash">#k3</a></p>&mdash; whitney erin boesel (@weboesel) <a href="https://twitter.com/weboesel/status/721477628465704960">April 16, 2016</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>&hellip; has some crippling emotional consequences.</p>

  <p>As an exercise in empathy, try to imagine saying some of the terrible things people typed to each other online to a real person sitting directly in front of you. Or don't imagine, and <a href="http://www.esquire.com/sports/videos/a44351/female-sports-reporters-mean-tweets/">just watch this video</a>.</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/9tU-D-m2JY8" frameborder="0" allowfullscreen></iframe>

  <p>I challenge you to watch the entirety of that video. I couldn't do it. This is the second time I've tried, and I had to turn it off not even 2 minutes in because I couldn't take it any more.</p>

  <p>It's no coincidence that these are comments directed at women. Over the last few years I have come to understand how, as a straight white man, I have the privilege of being immune from most of this kind of treatment. But others are not so fortunate. The Guardian analyzed 70 million comments and found that <a href="https://www.theguardian.com/technology/2016/apr/12/the-dark-side-of-guardian-comments">online abuse is heaped disproportionately on women, people of color, and people of different sexual orientation</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/men-vs-women-comments-blocked.png" alt=""></p>

  <blockquote>
  <p>And avalanches happen easily online. Anonymity disinhibits people, making some of them more likely to be abusive. Mobs can form quickly: once one abusive comment is posted, others will often pile in, competing to see who can be the most cruel. This abuse can move across platforms at great speed – from Twitter, to Facebook, to blogposts – and it can be viewed on multiple devices – the desktop at work, the mobile phone at home. To the person targeted, it can feel like the perpetrator is everywhere: at home, in the office, on the bus, in the street.</p>
  </blockquote>

  <p>I've only had a little taste of this treatment, once. The sense of being "under siege" &ndash; a constant barrage of vitriol and judgment pouring your way every day, every hour &ndash; was palpable. It was not pleasant. It absolutely affected my state of mind. Someone remarked in the comments that ultimately it did not matter, because as a white man I could walk away from the whole situation any time. And they were right. I began to appreciate what it would feel like when you <em>can't</em> walk away, when this harassment follows you around everywhere you go online, and you never really know when the next incident will occur, or exactly what shape it will take.</p>

  <p>Imagine the feeling of being constantly on edge like that, every day. What happens to your state of mind when walking away isn't an option? It gave me great pause.</p>

  <p><a href="http://www.brickartist.com/"><img src="https://blog.codinghorror.com/content/images/2016/04/the-lego-scream.jpg" alt="The Scream by Nathan Sawaya" title=""></a></p>

  <p>I admired the way Stephanie Wittels Wachs actually <em>engaged</em> with the person who left that awful comment. This is a man who has two children of his own, and should be no stranger to the kind of pain involved in a child's death. And yet he felt the need to post the word "Junkie" in reply to a mother's anguish over losing her child to drug addiction. </p>

  <blockquote>
  <p>Isn’t this what empathy is? Putting myself in someone else’s shoes with the knowledge and awareness that I, too, am human and, therefore, susceptible to this tragedy or any number of tragedies along the way?</p>
  </blockquote>

  <p>Most would simply delete the comment, block the user, and walk away. Totally defensible. But she didn't. She takes the time and effort to attempt to understand this person who is abusing her mother, to reach them, to connect, to <strong>demonstrate the very empathy this man appears incapable of</strong>.</p>

  <p>Consider the related story of Lenny Pozner, who lost a child at Sandy Hook, and became <a href="https://www.washingtonpost.com/news/grade-point/wp/2016/01/13/the-father-of-a-boy-killed-at-sandy-hook-gets-death-threats-from-people-who-say-the-shooting-was-a-hoax/">the target of groups who believe the event was a hoax</a>, and similarly selflessly devotes much of his time to refuting and countering these bizarre claims.</p>

  <blockquote>
  <p>Tracy’s alleged harassment was hardly the first, Pozner said. There’s a whole network of people who believe the media reported a mass shooting that never happened, he said, that the tragedy was an elaborate hoax designed to increase support for gun control. Pozner said he gets ugly comments often on social media, such as, “Eventually you’ll be tried for your crimes of treason against the people,” “… I won’t be satisfied until the caksets are opened…” and “How much money did you get for faking all of this?”</p>
  </blockquote>

  <p>It's easy to practice empathy when you limit it to people that are easy to empathize with &ndash; the downtrodden, the undeserving victims. But <strong>it is another matter entirely to empathize with those that hate, harangue, and intentionally make other people's lives miserable</strong>. If you can do this, you are a far better person than me. I struggle with it. But my hat is off to you. There's no better way to teach empathy than to practice it, in the most difficult situations.</p>

  <p>In individual cases, reaching out and really trying to empathize with people you disagree with or dislike can work, even people who happen to be lifelong members of hate organizations, <a href="http://www.newyorker.com/magazine/2015/11/23/conversion-via-twitter-westboro-baptist-church-megan-phelps-roper">as in the remarkable story of Megan Phelps-Roper</a>:</p>

  <blockquote>
  <p>As a member of the Westboro Baptist Church, in Topeka, Kansas, Phelps-Roper believed that AIDS was a curse sent by God. She believed that all manner of other tragedies—war, natural disaster, mass shootings—were warnings from God to a doomed nation, and that it was her duty to spread the news of His righteous judgments. To protest the increasing acceptance of homosexuality in America, the Westboro Baptist Church picketed the funerals of gay men who died of AIDS and of soldiers killed in Iraq and Afghanistan. Members held signs with slogans like “GOD HATES FAGS” and “THANK GOD FOR DEAD SOLDIERS,” and the outrage that their efforts attracted had turned the small church, which had fewer than a hundred members, into a global symbol of hatred.</p>
  </blockquote>

  <p>Perhaps one of the greatest failings of the Internet is <a href="https://medium.com/@jeremypreacher/emotional-labor-and-diversity-in-community-management-eb3a4985d71a#.2ei4skfxv">the breakdown in cost of emotional labor</a>.</p>

  <blockquote>
  <p>First we’ll reframe the problem: the real issue is not Problem Child’s opinions &ndash; he can have whatever opinions he wants. The issue is that he’s doing zero emotional labor &ndash; he’s not thinking about his audience or his effect on people at all. (Possibly, he’s just really bad at modeling other people’s responses &ndash; the outcome is the same whether he lacks the will or lacks the skill.) But to be a good community member, he <em>needs</em> to consider his audience. </p>
  </blockquote>

  <p>True empathy means reaching out and engaging in a loving way with everyone, even those that are hurtful, hateful, or spiteful. But on the Internet, can you do it every day, multiple times a day, across hundreds of people? Is this a reasonable thing to ask of someone? Is it even <em>possible</em>, short of sainthood?</p>

  <p>The question remains: <strong>why would people post such hateful things</strong> in the first place? Why reply "Junkie" to a mother's anguish? Why ask the father of a murdered child to publicly prove his child's death was not a hoax? Why tweet "Thank God for AIDS!"</p>

  <p>Unfortunately, I think I know the answer to this question, and you're not going to like it. </p>

  <p><a href="http://www.webtoons.com/en/comedy/bluechair/ep-59-busy-work/viewer?title_no=199&amp;episode_no=61"><img src="https://blog.codinghorror.com/content/images/2016/04/dont-let-the-existential-dread.jpg" alt="Busy-Work by Shen, owlturd.com" title=""></a></p>

  <p>I don't like it. I don't want it. But I know.</p>

  <p>I have laid some heavy stuff on you in this post, and for that, I apologize. I think the weight of what I'm trying to communicate here requires it. I have to warn you that the next article I'm about to link is far heavier than anything I have posted above, maybe the heaviest thing I've ever posted. It's about the legal quandary presented in the tragic cases of children who died because their parents accidentally left them strapped into carseats, and it won a much deserved pulitzer. It is also <a href="https://www.washingtonpost.com/lifestyle/magazine/fatal-distraction-forgetting-a-child-in-thebackseat-of-a-car-is-a-horrifying-mistake-is-it-a-crime/2014/06/16/8ae0fe3a-f580-11e3-a3a5-42be35962a52_story.html">one of the most harrowing things I have ever read</a>.</p>

  <blockquote>
  <p>Ed Hickling believes he knows why. Hickling is a clinical psychologist from Albany, N.Y., who has studied the effects of fatal auto accidents on the drivers who survive them. He says these people are often judged with disproportionate harshness by the public, even when it was clearly an accident, and even when it was indisputably not their fault.</p>
  
  <p>Humans, Hickling said, have a fundamental need to create and maintain a narrative for their lives in which the universe is not implacable and heartless, that terrible things do not happen at random, and that catastrophe can be avoided if you are vigilant and responsible.</p>
  
  <p>In hyperthermia cases, he believes, the parents are demonized for much the same reasons. “We are vulnerable, but we don’t want to be reminded of that. We want to believe that the world is understandable and controllable and unthreatening, that if we follow the rules, we’ll be okay. <strong>So, when this kind of thing happens to other people, we need to put them in a different category from us. We don’t want to resemble them, and the fact that we might is too terrifying to deal with. So, they have to be monsters.</strong>”</p>
  </blockquote>

  <p>This man left the junkie comment because he is afraid. He is afraid his own children could become drug addicts. He is afraid his children, through no fault of his, through no fault of anyone at all, could die at 30. When presented with real, tangible evidence of the pain and grief a mother feels at the drug related death of her own child, and the reality that it could happen to anyone, it became so overwhelming that it was too much for him to bear.</p>

  <p>Those "Sandy Hook Truthers" harass the father of a victim because they are afraid. They are afraid their own children could be viciously gunned down in cold blood any day of the week, bullets tearing their way through the bodies of the teachers standing in front of them, desperately trying to protect them from being murdered. They can't do anything to protect their children from this, and in fact there's <em>nothing any of us can do</em> to protect our children from being murdered at random, at school any day of the week, at the whim of any mentally unstable individual with access to an assault rifle. That's the harsh reality.</p>

  <p>When faced with the abyss of pain and grief that parents feel over the loss of their children, due to utter random chance in a world they can't control, they could never control, maybe none of us can <em>ever</em> control, the overwhelming sense of existential dread is simply too much to bear. So <strong><em>they have to be monsters</em></strong>. They must be.</p>

  <p>And we will fight these monsters, tooth and nail, raging in our hatred, so we can forget our pain, at least for a while.</p>

  <blockquote>
  <p>After Lyn Balfour’s acquittal, this comment appeared on the Charlottesville News Web site:</p>
  
  <p>“If she had too many things on her mind then she should have kept her legs closed and not had any kids. They should lock her in a car during a hot day and see what happens.”</p>
  </blockquote>

  <p>I imagine the suffering that these parents are already going through, reading these words that another human being typed to them, just <em>typed</em>, and something breaks inside me. I can't process it. But rather than pitting ourselves against each other out of fear, recognize that the monster who posted this terrible thing is me. It's you. It's all of us.</p>

  <p>The weight of seeing through the fear and beyond the monster to simply discover <em>yourself</em> is often too terrible for many people to bear. In a world of heavy things, it's the heaviest there is. </p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] At Stack Overflow, we help developers learn, share, and grow. Whether you’re looking for your next dream job or looking to build out your team, <a href="http://careers.stackoverflow.com" rel="nofollow">we've got your back</a>.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Here's The Programming Game You Never Asked For]]></title>
  <description><![CDATA[<p>You know what's universally regarded as un-fun by most programmers? Writing <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly language code</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/assembly-language.png" alt=""></p>

  <p>As Steve McConnell <a href="http://www.amazon.com/dp/0735619670/?tag=codihorr-20">said back in 1994</a>:</p>

  <blockquote>
  <p>Programmers working with high-level languages achieve better productivity and quality than those working with lower-level languages. Languages such as C++, Java, Smalltalk, and Visual Basic have been credited with</p></blockquote>]]></description>
  <link>https://blog.codinghorror.com/heres-the-programming-game-you-never-asked-for/</link>
  <guid isPermaLink="false">fd0fdff9-caa5-470f-a8d2-ed99b340e3f8</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 15 Apr 2016 09:48:18 GMT</pubDate>
  <content:encoded><![CDATA[<p>You know what's universally regarded as un-fun by most programmers? Writing <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly language code</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/assembly-language.png" alt=""></p>

  <p>As Steve McConnell <a href="http://www.amazon.com/dp/0735619670/?tag=codihorr-20">said back in 1994</a>:</p>

  <blockquote>
  <p>Programmers working with high-level languages achieve better productivity and quality than those working with lower-level languages. Languages such as C++, Java, Smalltalk, and Visual Basic have been credited with improving productivity, reliability, simplicity, and comprehensibility by factors of 5 to 15 over low-level languages such as assembly and C. <strong>You save time when you don't need to have an awards ceremony every time a C statement does what it's supposed to.</strong></p>
  </blockquote>

  <p>Assembly is a language where, for performance reasons, every individual command is communicated in excruciating low level detail directly to the CPU. As we've gone from fast CPUs, to faster CPUs, to multiple absurdly fast CPU cores on the same die, to "gee, we kinda stopped caring about CPU performance altogether five years ago", there hasn't been much need for the kind of hand-tuned performance you get from assembly. Sure, there are <a href="http://blog.codinghorror.com/i-happen-to-like-heroic-coding/">the occasional heroics</a>, and they are amazing, but in terms of Getting Stuff Done, assembly has been well off the radar of mainstream programming for probably twenty years now, and for good reason.</p>

  <p>So who in their right mind would take up tedious assembly programming today? Yeah, nobody. But wait! What if I told you your Uncle Randy had just died and left behind this mysterious old computer, <a href="http://www.zachtronics.com/tis-100/">the TIS-100?</a></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/aunt-doris-note-tis-100.jpg" alt=""></p>

  <p>And what if I also told you the only way to figure out what that TIS-100 computer was used for &ndash; and what good old Uncle Randy was up to &ndash; was to read a (blessedly short 14 page) photocopied reference manual and fix its corrupted boot sequence &hellip; <em>using assembly language?</em></p>

  <p><a href="http://www.zachtronics.com/tis-100/"><img src="https://blog.codinghorror.com/content/images/2016/04/tis-100-level-one.png" alt="" title=""></a></p>

  <p>Well now, by God, it's time to learn us some assembly and get to the bottom of this mystery, isn't it? As its <a href="http://www.zachtronics.com/">creator</a> notes, <strong>this is the assembly language programming game you never asked for!</strong></p>

  <p>I was surprised to discover my co-founder <a href="https://eviltrout.com/">Robin Ward</a> liked TIS-100 so much that he not only played the game (presumably to completion) but wrote <a href="https://github.com/eviltrout/tis-100">a TIS-100 emulator in C</a>. This is apparently the kind of thing he does for fun, in his free time, when he's not already working full time with us programming <a href="http://www.discourse.org">Discourse</a>. Programmers gotta &hellip; program.</p>

  <p>Of course there's a long history of programming games. What makes TIS-100 unique is the way it fetishizes assembly programming, while most programming games take it a bit easier on you by easing you in with general concepts and simpler abstractions. But even "simple" programming games can be quite difficult. Consider one of my favorites on the Apple II, <a href="https://en.wikipedia.org/wiki/Rocky%27s_Boots">Rocky's Boots</a>, and its sequel, Robot Odyssey. <a href="http://blog.codinghorror.com/programming-4-fun/">I loved this game</a>, but in true programming fashion it was so difficult that <a href="http://www.slate.com/articles/technology/bitwise/2014/01/robot_odyssey_the_hardest_computer_game_of_all_time.html">finishing it in any meaningful sense was basically impossible</a>:</p>

  <blockquote>
  <p>Let me say: Any kid who completes this game while still a kid (I know only one, who also is one of the smartest programmers I’ve ever met) is guaranteed a career as a software engineer. Hell, any adult who can complete this game should go into engineering. <strong>Robot Odyssey is the hardest damn “educational” game ever made.</strong> It is also a stunning technical achievement, and one of the most innovative games of the Apple IIe era.</p>
  
  <p><img src="https://blog.codinghorror.com/content/images/2016/04/robot-odyssey.png" alt="" title=""></p>
  
  <p>Visionary, absurdly difficult games such as this gain cult followings. It is the game I remember most from my childhood. It is the game I love (and despise) the most, because it was the hardest, the most complex, the most challenging. The world it presented was like being exposed to Plato’s forms, a secret, nonphysical realm of pure ideas and logic. The challenge of the game—and it was one serious challenge—was to understand that other world. Programmer Thomas Foote had just started college when he picked up the game: “I swore to myself,” he told me, “that as God is my witness, I would finish this game before I finished college. I managed to do it, but just barely.”</p>
  </blockquote>

  <p>I was happy dinking around with a few robots that did a few things, got stuck, and moved on to other games. I got a little turned off by the way it treated programming as electrical engineering; messing around with a ton of AND OR and NOT gates was just not my jam. I was already <a href="http://blog.codinghorror.com/everything-i-needed-to-know-about-programming-i-learned-from-basic/">cutting my teeth on BASIC by that point</a> and I sensed a level of mastery was necessary here that I probably didn't have and I wasn't sure I even <em>wanted</em>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/robot-odyssey-chip.png" alt=""></p>

  <p>I'll take a COBOL code listing over <em>that</em> monstrosity any day of the week. Perhaps Robot Odyssey was so hard because, in the end, it was a bare metal CPU programming simulation, like TIS-100.</p>

  <p>A more gentle example of a modern programming game is Tomorrow Corporation's excellent <a href="http://tomorrowcorporation.com/humanresourcemachine">Human Resource Machine</a>.</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/428R_oEjGGI" frameborder="0" allowfullscreen></iframe>

  <p>It has exactly the irreverent sense of humor you'd expect from the studio that built World of Goo and Little Inferno, both excellent and highly recommendable games in their own right. If you've ever wanted to find out if someone is truly interested in programming, recommend this game to them and see. It starts with only 2 instructions and slowly widens to include 11. Corporate drudgery has never been so &hellip; er, fun?</p>

  <p>I'm thinking about this because I believe there's a strong connection between programming games and being a talented software engineer. It's that essential sense of <em>play</em>, the idea that you're experimenting with this stuff because you enjoy it, and you bend it to your will out of the sheer joy of creation more than anything else. As I <a href="http://blog.codinghorror.com/programming-love-it-or-leave-it/">once said</a>:</p>

  <blockquote>
  <p>Joel implied that good programmers love programming so much they'd do it for <em>no pay at all</em>. I won't go quite that far, but I will note that the best programmers I've known have all had a <strong>lifelong passion for what they do</strong>. There's no way a minor economic blip would ever convince them they should do anything else. No way. No how.</p>
  </blockquote>

  <p>I'd rather sit a potential hire in front of Human Resource Machine and time how long it takes them to work through a few levels than <a href="http://blog.codinghorror.com/why-cant-programmers-program/">have them solve FizzBuzz for me</a> on a whiteboard. Is this interview about demonstrating competency in a certain technical skill that's worth a certain amount of money, or showing me how you can <em>improvise and have fun?</em></p>

  <p>That's why I was so excited when Patrick, Thomas, and Erin founded <a href="https://www.starfighters.io/">Starfighter</a>.</p>

  <p><a href="https://www.starfighters.io/"><img src="https://blog.codinghorror.com/content/images/2016/04/starfighter-logo.png" alt="" title=""></a></p>

  <p>If you want to know how competent a programmer is, give them a real-ish simulation of a real-ish system to hack against and experiment with &ndash; and see how far they get. In security parlance, this is known <a href="https://www.defcon.org/html/links/dc-ctf.html">as a CTF</a>, as popularized by Defcon. But it's rarely extended to programming, until now. Their first simulation is <a href="https://www.stockfighter.io/">StockFighter</a>.</p>

  <p>Participants are given:</p>

  <ul>
  <li>An interactive trading blotter interface</li>
  <li>A real, functioning set of limit-order-book venues</li>
  <li>A carefully documented JSON HTTP API, with an API explorer</li>
  <li>A series of programming missions.</li>
  </ul>

  <p>Participants are asked to:</p>

  <ul>
  <li>Implement programmatic trading against a real exchange in a thickly traded market.</li>
  <li>Execute block-shopping trading strategies.</li>
  <li>Implement electronic market makers.</li>
  <li>Pull off an elaborate HFT trading heist.</li>
  </ul>

  <p>This is a <em>seriously</em> next level hiring strategy, far beyond anything else I've seen out there. It's so next level that to be honest, I got really jealous reading about it, because <strong>I've felt for a long time that Stack Overflow should be doing yearly programming game events exactly like this</strong>, with special one-time badges obtainable only by completing certain levels on that particular year. Stack Overflow is <a href="http://blog.codinghorror.com/for-a-bit-of-colored-ribbon/">already a sort of game</a>, but people would go <em>nuts</em> for a yearly programming game event. Absolutely <em>bonkers</em>.</p>

  <p>I know we've talked about <a href="http://blog.codinghorror.com/we-hire-the-best-just-like-everyone-else/">giving lip service to the idea of hiring the best</a>, but if that's <em>really</em> what you want to do, the best programmers I've ever known have excelled at exactly the situation that Starfighter simulates &mdash; live troubleshooting and reverse engineering of an existing system, even to the point of <a href="http://blog.codinghorror.com/why-isnt-my-encryption-encrypting/">finding rare exploits</a>. </p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/04/stockfighter-hardware.jpg" alt=""></p>

  <p>Consider the dedication of this participant who built <a href="https://discuss.starfighters.io/t/wireless-handheld-build-done/4985?u=codinghorror">a complete wireless trading device</a> for StockFighter. Was it necessary? Was it practical? No. <strong>It's the programming game we never asked for.</strong> But here we are, regardless.</p>

  <p>An arbitrary programming game, particularly one that goes to great lengths to simulate a fictional system, is a wonderful expression of the inherent joy in <em>playing</em> and <em>experimenting</em> with code. If I could find them, I'd gladly hire a dozen people just like that any day, and set them loose on our very real programming project.</p>

  <table>  
  <tr><td class="welovecodinghorror">[advertisement] At Stack Overflow, we put developers first. We already help you find answers to your tough coding questions; now let us help you <a href="http://careers.stackoverflow.com" rel="nofollow">find your next job</a>.</td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Thanks For Ruining Another Game Forever, Computers]]></title>
  <description><![CDATA[<p>In 2006, after <a href="http://blog.codinghorror.com/chess-computer-v-human/">visiting the Computer History Museum's exhibit on Chess</a>, I opined:</p>

  <blockquote>
  <p>We may have reached an inflection point. The problem space of chess is so astonishingly large that incremental increases in hardware speed and algorithms are unlikely to result in meaningful gains from here on out.</p>
  </blockquote>

  <p>So. About</p>]]></description>
  <link>https://blog.codinghorror.com/thanks-for-ruining-another-game-forever-computers/</link>
  <guid isPermaLink="false">42ef9ce8-9e98-4741-bf61-fc0402be18d2</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 25 Mar 2016 22:29:41 GMT</pubDate>
  <content:encoded><![CDATA[<p>In 2006, after <a href="http://blog.codinghorror.com/chess-computer-v-human/">visiting the Computer History Museum's exhibit on Chess</a>, I opined:</p>

  <blockquote>
  <p>We may have reached an inflection point. The problem space of chess is so astonishingly large that incremental increases in hardware speed and algorithms are unlikely to result in meaningful gains from here on out.</p>
  </blockquote>

  <p>So. About that. Turns out I was kinda &hellip; <em>totally completely wrong</em>. The number of possible moves, or "problem space", of Chess is indeed astonishingly large, estimated to be 10<sup>50</sup>:</p>

  <blockquote>
  <p>100,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000</p>
  </blockquote>

  <iframe width="420" height="315" src="https://www.youtube.com/embed/iSnAiXKU7h8" frameborder="0" allowfullscreen></iframe>

  <p><a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)">Deep Blue</a> was interesting because it forecast a particular kind of future, a future where <strong>specialized hardware enabled brute force attack of the enormous chess problem space</strong>, as its purpose built chess hardware outperformed general purpose CPUs of the day by <em>many</em> orders of magnitude. How many orders of magnitude? In the heady days of 1997, Deep Blue could evaluate <strong>200 million chess positions per second</strong>. And that was enough to defeat Kasparov, the highest ever ranked human player &ndash; until <a href="https://en.wikipedia.org/wiki/Comparison_of_top_chess_players_throughout_history">2014</a> at least. Even though one of its best moves was <a href="http://www.wired.com/2012/09/deep-blue-computer-bug/">the result of a bug</a>.</p>

  <blockquote>
  <p>200,000,000</p>
  </blockquote>

  <p>In 2006, about ten years later, according to the <a href="http://www.chess.com/download/view/fritz-12-benchmark">Fritz Chess benchmark</a>, my PC could evaluate only 4.5 million chess positions per second.</p>

  <blockquote>
  <p>4,500,000</p>
  </blockquote>

  <p>Today, about twenty years later, that very same benchmark says my PC can evaluate a mere 17.2 million chess positions per second.</p>

  <blockquote>
  <p>17,200,000</p>
  </blockquote>

  <p>Ten years, four times faster. Not bad! Part of that is I went from dual to quad core, and these chess calculations scale almost linearly with the number of cores. An eight core CPU, no longer particularly exotic, could probably achieve ~28 million on this benchmark today.</p>

  <blockquote>
  <p>28,000,000</p>
  </blockquote>

  <p>I am not sure the scaling is exactly linear, but it's fair to say that even now, <em>twenty years later</em>, a modern 8 core CPU is still about an order of magnitude slower at the brute force task of evaluating chess positions than what Deep Blue's specialized chess hardware achieved in 1997.</p>

  <p>But here's the thing: none of that speedy brute forcing matters today. Greatly improved chess programs running on mere <em>handheld devices</em> can perform <a href="https://en.wikipedia.org/wiki/Human%E2%80%93computer_chess_matches#Pocket_Fritz_4_.282009.29">beyond grandmaster level</a>. </p>

  <blockquote>
  <p>In 2009 a chess engine running on slower hardware, a 528 MHz HTC Touch HD mobile phone running Pocket Fritz 4 reached the grandmaster level &ndash; it won a category 6 tournament with a performance rating of 2898. <strong>Pocket Fritz 4 searches fewer than 20,000 positions per second. This is in contrast to supercomputers such as Deep Blue that searched 200 million positions per second.</strong></p>
  </blockquote>

  <p>As far as chess goes, despite what I so optimistically thought in 2006, it's been <em>game over</em> for humans for quite a few years now. The best computer chess programs, vastly more efficient than Deep Blue, combined with modern CPUs which are now finally within an order of magnitude of what Deep Blue's specialized chess hardware could deliver, play at levels way beyond what humans can achieve.</p>

  <p><strong>Chess: ruined forever.</strong> Thanks, computers. You jerks.</p>

  <p>Despite this resounding defeat, there was still hope for humans in the game of Go. The number of possible moves, or "problem space", of Go is estimated to be 10<sup>170</sup>:</p>

  <blockquote>
  <p>1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000</p>
  </blockquote>

  <p>Remember that Chess had a mere <em>fifty</em> zeroes there? Go has <a href="https://en.wikipedia.org/wiki/Go_and_mathematics">more possible moves than there are <strong>atoms in the universe</strong></a>.</p>

  <p>Wrap your face around that one.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/03/go-game.jpg" alt=""></p>

  <p>Deep Blue was a statement about the inevitability of <em>eventually</em> being able to brute force your way around a difficult problem with the constant wind of <a href="http://blog.codinghorror.com/moores-law-in-practical-terms/">Moore's Law</a> at your back. If Chess is the quintessential European game, Go is the quintessential Asian game. Go requires a completely different strategy. Go means wrestling with a problem that is essentially <a href="http://ai-depot.com/LogicGames/Go-Complexity.html">impossible for computers to solve in any traditional way</a>.</p>

  <blockquote>
  <p>A simple material evaluation for chess works well &ndash; each type of piece is given a value, and each player receives a score depending on his/her remaining pieces. The player with the higher score is deemed to be 'winning' at that stage of the game. </p>
  
  <p>However, Chess programmers innocently asking Go players for an evaluation function would be met with disbelief! No such simple evaluation exists. Since there is only a single type of piece, only the number each player has on the board could be used for a simple material heuristic, and there is almost no discernible correlation between the number of stones on the board and what the end result of the game will be.</p>
  </blockquote>

  <p>Analysis of a problem this hard, with brute force completely off the table, is colloquially called "AI", though that term is a bit of a stretch to me. I prefer to think of it as building systems that can learn from experience, aka <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>. Here's a talk which covers DeepMind learning to play classic Atari 2600 videogames. (Jump to the 10 minute mark to see what I mean.)</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/rbsqaJwpu6A" frameborder="0" allowfullscreen></iframe>

  <p>As impressive as this is &ndash; and it truly is &ndash; bear in mind that games as simple as Pac-Man still remain far beyond the grasp of Deep Mind. But what happens when you point a system like that at the game of Go?</p>

  <p>DeepMind built a system, <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>, designed to see how far they could get with those approaches in the game of Go. AlphaGo recently played one of the best Go players in the world, Lee Sedol, and <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">defeated him in a stunning 4-1 display</a>. Being the optimist that I am, I guessed that DeepMind would win one or two games, but a near total rout like this? Incredible. <strong>In the space of just 20 years, computers went from barely beating the best humans at Chess, with a problem space of 10<sup>50</sup>, to definitively beating the best humans at Go, with a problem space of 10<sup>170</sup>.</strong> How did this happen? </p>

  <p>Well, a few things happened, but one unsung hero in this transformation is the humble video card, or GPU. </p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/03/asus-380x-front.jpg" alt=""></p>

  <p>Consider this breakdown of <a href="https://en.m.wikipedia.org/wiki/FLOPS#Hardware_costs">the cost of floating point operations</a> over time, measured in <strong>dollars per gigaflop</strong>:</p>

  <table style="width:320px">  
  <tr><td>1961</td><td>$8,300,000,000</td><td>  
  </td></tr><tr><td>1984</td><td>$42,780,000</td><td>  
  </td></tr><tr><td>1997</td><td>$42,000</td><td>  
  </td></tr><tr><td>2000</td><td>$1,300</td><td>  
  </td></tr><tr><td>2003</td><td>$100</td><td>  
  </td></tr><tr><td>2007</td><td>$52</td><td>  
  </td></tr><tr><td>2011</td><td>$1.80</td><td>  
  </td></tr><tr><td>2012</td><td>$0.73</td><td>  
  </td></tr><tr><td>2013</td><td>$0.22</td><td>  
  </td></tr><tr><td>2015</td><td>$0.08</td><td>  
  </td></tr></table>

  <p>What's not clear in this table is that after 2007, <strong>all the big advances in FLOPS came from gaming video cards</strong> designed for high speed real time 3D rendering, and as an incredibly beneficial side effect, they also turn out to be <a href="https://blogs.nvidia.com/blog/2016/01/12/accelerating-ai-artificial-intelligence-gpus/">crazily fast at machine learning tasks</a>.</p>

  <blockquote>
  <p>The Google Brain project had just achieved amazing results — it learned to recognize cats and people by watching movies on YouTube. But it required 2,000 CPUs in servers powered and cooled in one of Google’s giant data centers. Few have computers of this scale. Enter NVIDIA and the GPU. Bryan Catanzaro in NVIDIA Research teamed with Andrew Ng’s team at Stanford to use GPUs for deep learning. As it turned out, 12 NVIDIA GPUs could deliver the deep-learning performance of 2,000 CPUs.</p>
  </blockquote>

  <p>Let's consider a related case of highly parallel computation. How much faster is a GPU at <a href="http://blog.codinghorror.com/speed-hashing/">password hashing</a>?</p>

  <table cellpadding="2" cellspacing="2" style="width:300px">  
  <tr><td>Radeon 7970</td><td align="right">8213.6 M c/s</td></tr>  
  <tr><td>6-core AMD CPU</td><td align="right">52.9 M c/s</td></tr>  
  </table>

  <p>Only <strong>155 times faster</strong> right out of the gate. No big deal. On top of that, CPU performance has largely stalled in the last decade. While more and more cores are placed on each die, which is great when the problems are parallelizable &ndash; as they definitely are in this case &ndash; the actual performance improvement of any individual core over the last 5 to 10 years is rather modest.</p>

  <p><strong>But GPUs are still doubling in performance every few years</strong>. Consider password hash cracking expressed in the rate of <a href="http://www.crackingservice.com/?q=node/20">hashes per second</a>:</p>

  <table style="width:320px">  
  <tr><td>GTX 295</td><td>2009</td><td>25k</td></tr>  
  <tr><td>GTX 690</td><td>2012</td><td>54k</td></tr>  
  <tr><td>GTX 780 Ti</td><td>2013</td><td>100k</td></tr>  
  <tr><td>GTX 980 Ti</td><td>2015</td><td>240k</td></tr>  
  </table>

  <p>The latter video card is the one in my machine right now. It's likely the next major revision from Nvidia, due later this year, will <a href="http://wccftech.com/nvidia-pascal-gpu-gtc-2015/">double these rates again</a>. </p>

  <p>(While I'm at it, I'd like to emphasize how much it sucks to be an 8 character password in today's world. <strong>If your password is only 8 characters, that's perilously close to no password at all.</strong> That's also why why <a href="http://blog.codinghorror.com/your-password-is-too-damn-short/">your password is (probably) too damn short</a>. In fact, we just raised the <em>minimum</em> allowed password length on <a href="http://www.discourse.org">Discourse</a> to 10 characters, because annoying password complexity rules are much less effective in reality than <a href="http://arstechnica.com/security/2013/06/password-complexity-rules-more-annoying-less-effective-than-length-ones/">simply requiring longer passwords</a>.)</p>

  <p><a href="https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/">Distributed AlphaGo</a> used 1202 CPUs and <strong>176 GPUs</strong>. While that doesn't sound like much, consider that as we've seen, each GPU can be up to 150 times faster at processing these kinds of highly parallel datasets &mdash; so those 176 GPUs were the equivalent of adding ~26,400 CPUs to the task. Or more!</p>

  <p>Even if you don't care about video games, they happen to have a profound accidental impact on machine learning improvements. <strong>Every time you see a new video card release, don't think "slightly nicer looking games" think "wow, hash cracking and AI just got 2&times; faster &hellip; again!"</strong></p>

  <p>I'm certainly not making the same mistake I did when looking at Chess in 2006. (And in my defense, I totally did not see the era of GPUs as essential machine learning aids coming, even though I am a gamer.) If AlphaGo was intimidating today, having soundly beaten the best human Go player in the world, it'll be no contest after a few more years of GPUs doubling and redoubling their speeds again. </p>

  <p>AlphaGo, broadly speaking, is the culmination of two very important trends in computing:</p>

  <ol>
  <li><p>Huge increases in parallel processing power driven by consumer GPUs and videogames, which started in 2007. So if you're a gamer, congratulations! You're part of the problem-slash-solution.</p></li>
  <li><p>We're beginning to build sophisticated (and combined) algorithmic approaches for entirely new problem spaces that are far too vast to even begin being solved by brute force methods alone. And these approaches clearly work, insofar as they mastered one of the hardest games in the world, one that many thought humans would never be defeated in.</p></li>
  </ol>

  <p>Great. <a href="http://www.newyorker.com/tech/elements/in-the-age-of-google-deepmind-do-the-young-go-prodigies-of-asia-have-a-future">Another game ruined forever by computers</a>. Jerks.</p>

  <p>Based on our experience with Chess, and now Go, we know that computers will continue to beat us at virtually every game we play, in the same way that dolphins will always swim faster than we do. But what if that very same human mind was capable of not only building the dolphin, but continually refining it until they arrived at the <a href="http://jacquesmattheij.com/another-way-of-looking-at-lee-sedol-vs-alphago">world's fastest minnow</a>? Where Deep Blue was the more or less inevitable end result of brute force computation, AlphaGo is the beginning of a whole new era of sophisticated problem solving against far more enormous problems. <strong>AlphaGo's victory is not a defeat of the human mind, but its greatest triumph.</strong></p>

  <p>(If you'd like to learn more about the powerful intersection of sophisticated machine learning algorithms and your GPU, read <a href="https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/">this excellent summary of AlphaGo</a> and then <a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner">download the DeepMind Atari learner</a> and try it yourself.)</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] <a href="http://careers.stackoverflow.com" rel="nofollow">Find a better job the Stack Overflow way</a> - what you need when you need it, no spam, and no scams.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[We Hire the Best, Just Like Everyone Else]]></title>
  <description><![CDATA[<p>One of the most common pieces of advice you'll get as a startup is this:</p>

  <blockquote>
  <p><strong>Only hire the best.</strong> The quality of the people that work at your company will be one of the biggest factors in your success &ndash; or failure.</p>
  </blockquote>

  <p>I've heard this advice over and over and</p>]]></description>
  <link>https://blog.codinghorror.com/we-hire-the-best-just-like-everyone-else/</link>
  <guid isPermaLink="false">0021de52-3433-4795-bcea-ac469b44e36c</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 04 Mar 2016 12:17:58 GMT</pubDate>
  <content:encoded><![CDATA[<p>One of the most common pieces of advice you'll get as a startup is this:</p>

  <blockquote>
  <p><strong>Only hire the best.</strong> The quality of the people that work at your company will be one of the biggest factors in your success &ndash; or failure.</p>
  </blockquote>

  <p>I've heard this advice over and over and over at startup events, to the point that I got a little sick of hearing it. It's not wrong. Putting aside the fact that every single other startup in the world who heard this same advice before you is already out there frantically doing everything they can to hire all the best people out from under you and everyone else, it is superficially true. A company staffed by a bunch of people who don't care about their work and aren't good at their jobs isn't exactly poised for success. But in a room full of people giving advice to startups, nobody wants to talk about the elephant in that room:</p>

  <blockquote>
  <p>It doesn't matter how good the people are at your company when you happen to be working on the wrong problem, at the wrong time, using the wrong approach.</p>
  </blockquote>

  <p>Most startups, statistically speaking, <a href="http://fortune.com/2014/09/25/why-startups-fail-according-to-their-founders/">are going to fail</a>.</p>

  <p><a href="https://www.cbinsights.com/blog/startup-failure-post-mortem/"><img src="https://blog.codinghorror.com/content/images/2016/03/top-20-reasons-startups-fail.png" alt="" title=""></a></p>

  <p>And they will fail regardless of whether they hired "the best" due to circumstances largely beyond their control. So in that context does maximizing for the best possible hires really make sense?</p>

  <p>Given the risks, I think maybe "hire the nuttiest risk junkie adrenaline addicted has-ideas-so-crazy-they-will-never-work people you can find" might actually be more practical startup advice. (Actually, now that I think about it, if that describes you, and you have serious Linux, Ruby, and JavaScript chops, perhaps you should email me.)</p>

  <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I told that person the same thing I tell all prospective job candidates: &quot;come with me if you want to live&quot;</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/602375505694720000">May 24, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>Okay, the goal is to increase your <em>chance</em> of success, <s>however small it may be</s>, therefore you should strive to hire the best. Seems reasonable, even noble in its way. But this pursuit of the best unfortunately comes with a serious dark side. Can anyone even tell me what "best" is? By what metrics? Judged by which results? How do we measure this? Who among us is suitable to judge others as the best at &hellip; what, exactly? Best is an extreme. Not pretty good, not very good, not excellent, but aiming for the crème de la crème, <a href="http://www.joelonsoftware.com/articles/HighNotes.html">the top 1% in the industry</a>.</p>

  <blockquote>
  <p>The real trouble with using a lot of mediocre programmers instead of a couple of good ones is that no matter how long they work, they never produce something as good as what the great programmers can produce.</p>
  </blockquote>

  <p>Pursuit of this extreme means <strong>hiring anyone less than the best becomes unacceptable, even harmful</strong>:</p>

  <blockquote>
  <p>In the Macintosh Division, we had a saying, “A players hire A players; B players hire C players” – meaning that great people hire great people. On the other hand, mediocre people hire candidates who are not as good as they are, so they can feel superior to them. (If you start down this slippery slope, you’ll soon end up with Z players; this is called The Bozo Explosion. It is followed by The Layoff.) &mdash; <a href="http://guykawasaki.com/the_art_of_recr-2/">Guy Kawasaki</a></p>
  </blockquote>

  <!-- -->

  <blockquote>
  <p>There is an opportunity cost to keeping someone when you could do better. At a startup, that opportunity cost may be the difference between success and failure. Do you give less than full effort to make your enterprise a success? As an entrepreneur, you sweat blood to succeed. Shouldn’t you have a team that performs like you do? Every person you hire who is not a top player is like having a leak in the hull. Eventually you will sink. &mdash; <a href="http://venturebeat.com/2013/02/06/why-hiring-b-players-will-kill-your-startup/">Jon Soberg</a></p>
  </blockquote>

  <!-- -->

  <blockquote>
  <p>Why am I so hardnosed about this? It’s because it is much, <em>much</em> better to reject a good candidate than to accept a bad candidate. A bad candidate will cost a lot of money and effort and waste other people’s time fixing all their bugs. Firing someone you hired by mistake can take months and be nightmarishly difficult, especially if they decide to be litigious about it. In some situations it may be completely impossible to fire anyone. Bad employees demoralize the good employees. And they might be bad programmers but really nice people or maybe they <em>really need this job</em>, so you can’t bear to fire them, or you can’t fire them without pissing everybody off, or whatever. It’s just a bad scene. </p>
  
  <p>On the other hand, if you reject a good candidate, I mean, I guess in some existential sense an injustice has been done, but, hey, if they’re so smart, don’t worry, they’ll get <em>lots</em> of good job offers. Don’t be afraid that you’re going to reject too many people and you won’t be able to find anyone to hire. During the interview, it’s not your problem. Of course, it’s important to seek out good candidates. But once you’re actually interviewing someone, pretend that you’ve got 900 more people lined up outside the door. Don’t lower your standards no matter how hard it seems to find those great candidates. &mdash; <a href="http://www.joelonsoftware.com/articles/GuerrillaInterviewing3.html">Joel Spolsky</a></p>
  </blockquote>

  <p>I don't mean to be critical of anyone I've quoted. I love Joel, we founded Stack Overflow together, and his advice about interviewing and hiring <a href="http://www.amazon.com/Smart-Gets-Things-Done-Technical/dp/1590598385/?tag=codihorr-20">remains some of the best in the industry</a>. It's hardly unique to express these sort of opinions in the software and startup field. I could have cited two dozen different articles and treatises about hiring that say the exact same thing: aim high and set out to hire the best, or <em>don't bother</em>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/03/How-do-you-avoid-a-bad-hire.jpeg" alt=""></p>

  <p>This risk of hiring not-the-best is so severe, so existential a crisis to the very survival of your company or startup, the hiring process has to become highly selective, even arduous. <strong>It is better to reject a good applicant <em>every single time</em> than accidentally accept one single mediocre applicant.</strong> If the interview process produces literally anything other than unequivocal "Oh my God, this person is unbelievably talented, we have to hire them", from every single person they interviewed with, right down the line, then it's an automatic NO HIRE. Every time.</p>

  <p>This level of strictness always made me uncomfortable. I'm not going to lie, it starts with my own selfishness. I'm pretty sure I wouldn't get hired at big, famous companies with legendarily difficult technical interview processes because, you know, <em>they only hire the best</em>. I don't think I am one of the best. More like <a href="http://blog.codinghorror.com/who-needs-talent-when-you-have-intensity/">cranky, tenacious, and outspoken</a>, to the point that I wake up most days not even wanting to work with myself.</p>

  <p>If your hiring attitude is that it's better to be possibly wrong a hundred times so you can be absolutely right one time, you're going to be primed to throw away a lot of candidates on pretty thin evidence.</p>

  <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Before cofounding GitHub I applied for an engineering job at Yahoo and didn’t get it. Don’t let other people discourage you.</p>&mdash; Chris Wanstrath (@defunkt) <a href="https://twitter.com/defunkt/status/469607846527520768">May 22, 2014</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;ve been twitter following the careers of people we interviewed but passed on at my last gig.<br><br>Turns out we were almost always wrong.</p>&mdash; Trek Glowacki (@trek) <a href="https://twitter.com/trek/status/692116840940716032">January 26, 2016</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>Perhaps worst of all, if the interview process is predicated on zero doubt, total confidence &hellip; maybe this candidate doesn't feel right because they don't look like you, dress like you, think like you, speak like you, or come from a similar background as you? Are you <strong>accidentally maximizing for hidden bias?</strong></p>

  <p>One of the best programmers I ever worked with was Susan Warren, an ex-Microsoft engineer who taught me about the <a href="https://web.archive.org/web/20051101203600/http://weblogs.asp.net/swarren/archive/2004/04/26/120366.aspx">People Like Us problem</a>, way back in 2004:</p>

  <blockquote>
  <p>I think there is a real issue around diversity in technology (and most other places in life).  I tend to think of it as the PLU problem.  Folk (including MVPs) tend to connect best with folks most like them ("People Like Us").  In this case, male MVPs pick other men to become MVPs.  It's just human nature.</p>
  
  <p>As one reply notes, diversity is good.  I'd go as far as to say it's awesome, amazing, priceless.  But it's hard to get to -- the classic chicken and egg problem -- if you rely on your natural tendencies alone.  In that case, if you want more female MVPs to be invited you need more female MVPs.  If you want more Asian-American MVPs to be invited you need more Asian-American MVPs, etc.  And the (cheap) way to break a new group in is via quotas.</p>
  
  <p>IMO, building diversity via quotas is bad because they are unfair.  Educating folks on why diversity is awesome and how to build it is the right way to go, but also far more costly.</p>
  </blockquote>

  <p>Susan was (and is) amazing. I learned so much working under her, and a big part of what made her awesome was that she was very much Not Like Me. But how could I have appreciated that before meeting her? The fact is that as human beings, we tend to prefer what's comfortable, and what's most comfortable of all is &hellip; well, People Like Us. The effect can be shocking because it's so subtle, so unconscious &ndash; and yet, surprisingly strong:</p>

  <ul>
  <li><p>Baseball cards held by a black hand consistently sold for <a href="https://www.washingtonpost.com/news/wonk/wp/2015/12/11/whites-earn-more-than-blacks-even-on-ebay/">twenty percent less</a> than those held by a white hand.</p></li>
  <li><p>Using screens to hide the identity of auditioning musicians increased women's probability of advancing from preliminary orchestra auditions <a href="http://www.nber.org/papers/w5903">by fifty percent</a>.</p></li>
  <li><p>Denver police officers and community members were shown rapidly displayed photos of black and white men, some holding guns, some holding harmless objects like wallets, and asked to press either the "Shoot" or "Don't Shoot" button as fast as they could for each image. Both the police and community members were <a href="http://www.motherjones.com/politics/2014/11/science-of-racism-prejudice">three times more likely to shoot black men</a>.</p></li>
  </ul>

  <p>It's not intentional, it's never intentional. That's the problem. I think our industry needs to shed this old idea that it's OK, even <em>encouraged</em> to turn away technical candidates for anything less than absolute 100% confidence at every step of the interview process. Because when you do, <strong>you are accidentally optimizing for implicit bias</strong>. Even as a white guy who probably fulfills every stereotype you can think of about programmers, and who is in fact <a href="https://twitter.com/codinghorror/status/644616067847880705">wearing an "I Rock at Basic" t-shirt</a> while writing this very blog post*, that's what has always bothered me about it, more than the strictness. If you care at all about diversity in programming and tech, on any level, this hiring approach is not doing anyone any favors, and hasn't been. For years.</p>

  <p>I know what you're thinking.</p>

  <blockquote>
  <p>Fine, Jeff, if you're so smart, and "hiring the best" isn't the right strategy for startups, and maybe even harmful to our field as a whole, what <em>should</em> be doing? </p>
  </blockquote>

  <p>Well, I don't know, exactly. I may be the wrong person to ask because <a href="http://firstround.com/review/Heres-Why-Youre-Not-Hiring-the-Best-and-the-Brightest/">I'm also a big believer in <em>geographic</em> diversity on top of everything else</a>. Here's what the composition of the current Discourse team looks like:</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/03/discourse-team-timezones.png" alt=""></p>

  <p>I would argue, quite strongly and at some length, that if you want better diversity in the field, perhaps a good starting point is <strong>not demanding that all your employees live within a tiny 30 mile radius of San Francisco or Palo Alto.</strong> There's a whole wide world of Internet out there, full of amazing programmers at every level of talent and ability. Maybe broaden your horizons a little, even stretch said horizons outside the United States, <a href="https://ma.tt/2014/12/how-paul-graham-is-wrong/">if you can imagine such a thing</a>.</p>

  <p>I know hiring people is difficult, even with the very best of intentions and under ideal conditions, so I don't mean to trivialize the challenge. I've recommended plenty of things in the past, a smorgasboard of approaches to try or leave on the table as you see fit:</p>

  <ul>
  <li><a href="http://blog.codinghorror.com/on-interviewing-programmers/">On Interviewing Programmers</a></li>
  <li><a href="http://blog.codinghorror.com/the-nonprogramming-programmer/">The Non-Programming Programmer</a></li>
  <li><a href="http://blog.codinghorror.com/how-to-hire-a-programmer/">How to Hire a Programmer</a></li>
  <li><a href="http://blog.codinghorror.com/the-years-of-experience-myth/">The years of experience myth</a></li>
  </ul>

  <p>&hellip; but the one thing I keep coming back to, that I believe has enduring value in almost all situations, is <a href="https://hbr.org/2014/04/the-ceo-of-automattic-on-holding-auditions-to-build-a-strong-team">the audition project</a>:</p>

  <blockquote>
  <p>The most significant shift we’ve made is requiring every final candidate to work with us for three to eight weeks on a contract basis. Candidates do real tasks alongside the people they would actually be working with if they had the job. They can work at night or on weekends, so they don’t have to leave their current jobs; most spend 10 to 20 hours a week working with Automattic, although that’s flexible. (Some people take a week’s vacation in order to focus on the tryout, which is another viable option.) The goal is not to have them finish a product or do a set amount of work; it’s to allow us to quickly and efficiently assess whether this would be a mutually beneficial relationship. They can size up Automattic while we evaluate them. </p>
  </blockquote>

  <p>What I like about audition projects:</p>

  <ul>
  <li>It's real, practical work.</li>
  <li>They get paid. (Ask yourself who gets "paid" for a series of intensive interviews that lasts multiple days? Certainly not the candidate.)</li>
  <li>It's healthy to structure your work so that small projects like this can be taken on by outsiders. If you can't onboard a potential hire, you probably can't onboard a new hire very well either.</li>
  <li>Interviews, no matter how much effort you put into them, are so hit and miss that the only way to figure out if someone is <em>really</em> going to work in a given position is to <strong>actually work with them.</strong></li>
  </ul>

  <p>Every company says they want to hire the best. Anyone who tells you they know how to do that is either lying to you or to themselves. But I can tell you this: the companies that really <em>do</em> hire the best people in the world certainly don't accomplish that by hiring from the same tired playbook every other company in Silicon Valley uses.</p>

  <p>Try different approaches. Expand your horizons. Look beyond People Like Us and imagine what the world of programming could look like in 10, 20 or even 50 years &ndash; and help us move there by hiring to make it so.</p>

  <p>* And for the record, <a href="http://blog.codinghorror.com/everything-i-needed-to-know-about-programming-i-learned-from-basic/">I really do rock at BASIC</a>.</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] Building out your tech team? <a href="http://careers.stackoverflow.com/products" rel="nofollow">Stack Overflow Careers</a> helps you hire from the largest community for programmers on the planet. We built our site with developers like you in mind.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Is Your Computer Stable?]]></title>
  <description><![CDATA[<p>Over the last twenty years, I've probably built around a hundred computers. It's not very difficult, and in fact, it's gotten a whole lot easier over the years as computers become more highly integrated. Consider what it would take to build something very modern like <a href="http://blog.codinghorror.com/the-scooter-computer/">the Scooter Computer</a>:</p>

  <ol>
  <li>Apply a</li></ol>]]></description>
  <link>https://blog.codinghorror.com/is-your-computer-stable/</link>
  <guid isPermaLink="false">809c5cab-c7ea-4547-8d07-d0d41d56f831</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Sun, 14 Feb 2016 06:58:39 GMT</pubDate>
  <content:encoded><![CDATA[<p>Over the last twenty years, I've probably built around a hundred computers. It's not very difficult, and in fact, it's gotten a whole lot easier over the years as computers become more highly integrated. Consider what it would take to build something very modern like <a href="http://blog.codinghorror.com/the-scooter-computer/">the Scooter Computer</a>:</p>

  <ol>
  <li>Apply a dab of thermal compound to top of case.  </li>
  <li>Place motherboard in case.  </li>
  <li>Screw motherboard into case.  </li>
  <li>Insert SSD stick.  </li>
  <li>Insert RAM stick.  </li>
  <li>Screw case closed.  </li>
  <li>Plug in external power.  </li>
  <li>Boot.</li>
  </ol>

  <p>Bam done.</p>

  <iframe width="420" height="315" src="https://www.youtube.com/embed/PKf1I759PPQ" frameborder="0" allowfullscreen></iframe>

  <p>It's stupid easy. My six year old son and I have built Lego kits that were way more complex than this. Even a traditional desktop build is only a few more steps: insert CPU, install heatsink, route cables. And a server build is merely a few additional steps on top of that, maybe with some 1U or 2U space constraints. Scooter, desktop, or server, if you've built one computer, you've basically built them all.</p>

  <p>Everyone breathes a sigh of relief when their newly built computer boots up for the first time, no matter how many times they've done it before. But booting is only the beginning of the story. Yeah, it boots, great. Color me unimpressed. What we really need to know is whether <strong>that computer is stable</strong>. </p>

  <p>Although commodity computer parts <a href="https://www.pugetsystems.com/labs/articles/Most-Reliable-Hardware-of-2015-749/">are more reliable every year</a>, and vendors test their parts plenty before they ship them, there's no guarantee all those parts will work reliably <em>together</em>, in your particular environment, under your particular workload. And there's always the possibility, however slim, of getting very, very unlucky with subtly broken components.</p>

  <p>Because we're rational scientists, we test stuff in our native environment, and collect data to <strong>prove our computer is stable</strong>.  Right? So after we boot, we test.</p>

  <h5 id="memory">Memory</h5>

  <p>I like to start with memory tests, since those require bootable media and work the same on all x86 computers, even before you have an operating system. <a href="https://en.wikipedia.org/wiki/Memtest86">Memtest86</a> is the granddaddy of all memory testers. I'm not totally clear what caused the split between that and Memtest86+, but all of them work similarly. The one from passmark seems to be most up to date, so <a href="http://www.memtest86.com/download.htm">that's what I recommend</a>.</p>

  <p>Download the version of your choice, write it to a bootable USB drive, plug it into your newly built computer, boot and let it work its magic. It's all automatic. Just boot it up and watch it go.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/memtest86.png" alt=""></p>

  <p>(If your computer supports UEFI boot you'll get <a href="http://www.memtest86.com/support/ver_history.htm">the newest version 6.x</a>, otherwise you'll see version 4.2 as above.)</p>

  <p>I recommend <strong>one complete pass of memtest86</strong> at minimum, but if you want to be extra careful, let it run overnight. Also, if you have a lot of memory, memtest can take a while! For our servers with 128GB it took about three hours, and I expect that time scales linearly with the amount of memory.</p>

  <p>The "Pass" percentage at the top should get to 100% and the "Pass" count in the table should be greater than one. If you get any errors at all, anything whatsoever other than a clean 100% pass, <em>your computer is not stable</em>. Time to start removing RAM sticks and figure out which one is bad.</p>

  <h5 id="os">OS</h5>

  <p>All subsequent tests will require an operating system, and one basic iron clad test of stability for any computer is <strong>whether it can install an operating system</strong>. Pick your free OS of choice, and begin a default install. I recommend <a href="http://www.ubuntu.com/download/server">Ubuntu Server LTS x64</a> since it assumes less about your video hardware. Download the ISO and write it to a bootable USB drive. Then boot it.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/ubuntu-server-install.png" alt=""></p>

  <p>(Hey look it has a memory test option! How convenient!)</p>

  <ul>
  <li>Be sure you have network connected for the install with DHCP; it makes the install go faster when you don't have to wait for network detection to time out and nag you about the network stuff.</li>
  <li>In general, you'll be pressing <kbd>enter</kbd> a whole lot to accept all the defaults and proceed onward. I know, I know, we're installing Linux, but believe it or not, they've gotten the install bit down by now.</li>
  <li>About all you should be prompted for is the username and password of the default account. I recommend <code>jeff</code> and <code>password</code>, because I am one of the world's preeminent computer security experts.</li>
  <li>If you are installing from USB and get nagged about a missing CD, remove and reinsert the USB drive. No, I don't know why either, but <a href="http://askubuntu.com/questions/593002/fail-to-install-ubuntu-server-14-04-64bit-lts-from-usb-drive">it works</a>. </li>
  </ul>

  <p>If <em>anything</em> weird happens during your Ubuntu Server install that prevents it from finalizing the install and booting into Ubuntu Server &hellip; <em>your computer is not stable</em>. I know it doesn't sound like much, but this is a decent holistic test as it exercises the whole system in very repeatable ways.</p>

  <p>We'll need an OS installed for the next tests, anyway. I'm assuming you've installed Ubuntu, but any Linux distribution should work similarly.</p>

  <h5 id="cpu">CPU</h5>

  <p>Next up, let's make sure the brains of the operation are in order: the CPU. To be honest, if you've gotten this far, past the RAM and OS test, the odds of you having a completely broken CPU are fairly low. But we need to be <em>sure</em>, and the best way to do that is to call upon our old friend, Marin Mersenne.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/marin-mersenne.jpg" alt=""></p>

  <blockquote>
  <p>In mathematics, a Mersenne prime is a prime number that is one less than a power of two. That is, it is a prime number that can be written in the form M<sub><i>n</i></sub> = 2<sup><i>n</i></sup> − 1 for some integer <i>n</i>. They are named after Marin Mersenne, a French Minim friar, who studied them in the early 17th century. The first four Mersenne primes are 3, 7, 31, and 127.</p>
  </blockquote>

  <p>I've been using <a href="http://www.mersenne.org/download/">Prime95 and MPrime</a> &ndash; tools that attempt to rip through as many giant numbers as fast as possible to determine if they are prime &ndash; for the last 15 years. Here's how to download and install <code>mprime</code> on that fresh new Ubuntu Server system you just booted up.</p>

  <pre><code>mkdir mprime
  cd mprime
  wget ftp://mersenne.org/gimps/p95v287.linux64.tar.gz
  tar xzvf p95v287.linux64.tar.gz
  rm p95v287.linux64.tar.gz
  </code></pre>

  <p>(You may need to replace the version number in the above command with the current latest from the <a href="http://www.mersenne.org/download/">mersenne.org download page</a>, but as of this writing, that's the latest.)</p>

  <p>Now you have a copy of mprime in your user directory. Start it by typing <code>./mprime</code></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/mprime-prompt.png" alt=""></p>

  <p>Just passing through, thanks. Answer <kbd>N</kbd> to the GIMPS prompt.</p>

  <p>Next you'll be prompted for the number of torture test threads to run. They're smart here and always pick an equal number of threads to logical cores, so press <kbd>enter</kbd> to accept that. You want a full CPU test on all cores. Next, select the test type.</p>

  <ol>
  <li>Small FFTs (maximum heat and FPU stress, data fits in L2 cache, RAM not tested much).  </li>
  <li>In-place large FFTs (maximum power consumption, some RAM tested).  </li>
  <li>Blend (tests some of everything, lots of RAM tested).</li>
  </ol>

  <p>They're not kidding when they say "maximum power consumption", as you're about to learn. Select <kbd>2</kbd>. Then select <kbd>Y</kbd> to begin the torture and watch your CPU squirm in pain.</p>

  <pre><code>Accept the answers above? (Y):
  [Main thread Feb 14 05:48] Starting workers.
  [Worker #2 Feb 14 05:48] Worker starting
  [Worker #3 Feb 14 05:48] Worker starting
  [Worker #3 Feb 14 05:48] Setting affinity to run worker on logical CPU #2
  [Worker #4 Feb 14 05:48] Worker starting
  [Worker #2 Feb 14 05:48] Setting affinity to run worker on logical CPU #3
  [Worker #1 Feb 14 05:48] Worker starting
  [Worker #1 Feb 14 05:48] Setting affinity to run worker on logical CPU #1
  [Worker #4 Feb 14 05:48] Setting affinity to run worker on logical CPU #4
  [Worker #2 Feb 14 05:48] Beginning a continuous self-test on your computer.
  [Worker #4 Feb 14 05:48] Test 1, 44000 Lucas-Lehmer iterations of M7471105 using FMA3 FFT length 384K, Pass1=256, Pass2=1536.
  </code></pre>

  <p>Now's the time to break out your Kill-a-Watt or similar power consumption meter, if you have it, so you can <a href="http://blog.codinghorror.com/why-estimate-when-you-can-measure/">measure the maximum CPU power draw</a>. On most systems, unless you have an absolute beast of a gaming video card installed, the CPU is the single device that will pull the most heat and power in your system. This is full tilt, every core of your CPU burning as many cycles as possible. </p>

  <p>I suggest running the <code>i7z</code> utility from another console session so you can monitor core temperatures and speeds while <code>mprime</code> is running its torture test.</p>

  <pre><code>sudo apt-get install i7z
  sudo i7z
  </code></pre>

  <p><strong>Let mprime run overnight in maximum heat torture test mode</strong>. The Mersenne calculations are meticulously checked, so if there are any mistakes the whole process will halt with an error at the console. And if mprime halts, ever &hellip; <em>your computer is not stable.</em></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/i7z-status.png" alt=""></p>

  <p><strong>Watch those CPU temperatures!</strong> In addition to absolute CPU temperatures, you'll also want to keep an eye on total heat dissipation in the system. The system fans (if any) should spin up, and the whole system should be kept at reasonable temperatures through this ordeal, or else you're going to have a sick, overheating computer one day.</p>

  <p>The bad news is that it's extremely rare to have any kind of practical, real world workload remotely resembling the stress that Mersenne lays on your CPU. The good news is that if your system can survive the onslaught of Mersenne overnight, it's definitely ready for anything you can conceivably throw at it in the future.</p>

  <h5 id="disk">Disk</h5>

  <p>Disks are probably the easiest items to replace in most systems &ndash; and the ones most likely to fail over time. We know the disk can't be totally broken since we just installed an OS on the thing, but let's be sure.</p>

  <p>Start with a <a href="https://en.wikipedia.org/wiki/Badblocks">bad blocks test</a> for the whole drive.</p>

  <pre><code>sudo badblocks -sv /dev/sda
  </code></pre>

  <p>This exercises the full extent of the disk (in safe read only fashion). Needless to say, any errors here should prompt serious concern for that drive.</p>

  <pre><code>Checking blocks 0 to 125034839
  Checking for bad blocks (read-only test): done
  Pass completed, 0 bad blocks found. (0/0/0 errors)
  </code></pre>

  <p>Let's check the <a href="https://help.ubuntu.com/community/Smartmontools">SMART readings</a> for the drive next.</p>

  <pre><code>sudo apt-get install smartmontools
  smartctl -i /dev/sda 
  </code></pre>

  <p>That will let you know if the drive supports SMART. Let's enable it, if so, and see the basic drive stats:</p>

  <pre><code>smartctl -s on /dev/sda
  smartctl -a /dev/sda    
  </code></pre>

  <p>Now we can run some SMART tests. But first check how long the tests on offer will take:</p>

  <pre><code>smartctl -c /dev/sda
  </code></pre>

  <p>Run the <code>long</code> test if you have the time, or the <code>short</code> test if you don't:</p>

  <pre><code>smartctl -t long /dev/sda
  </code></pre>

  <p>It's done asynchronously, so after the time elapses, show the SMART test report and ensure you got a pass:</p>

  <pre><code>smartctl -l selftest /dev/sda 
  === START OF READ SMART DATA SECTION ===
  SMART Self-test log structure revision number 1
  Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error
  # 1  Extended offline    Completed without error       00%       100         -
  </code></pre>

  <p>Next, run a simple disk benchmark to see if you're getting roughly the performance you expect from the drive or array:</p>

  <pre><code>dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync
  hdparm -Tt /dev/sda
  </code></pre>

  <p>For a system with a basic SSD you should see results at least this good, and perhaps considerably better:</p>

  <pre><code>536870912 bytes (537 MB) copied, 1.52775 s, 351 MB/s
  Timing cached reads:   11434 MB in  2.00 seconds = 5720.61 MB/sec
  Timing buffered disk reads:  760 MB in  3.00 seconds = 253.09 MB/sec
  </code></pre>

  <p>Finally, let's try a more intensive test with <a href="https://en.wikipedia.org/wiki/Bonnie%2B%2B">bonnie++</a>, a disk benchmark:</p>

  <pre><code>sudo apt-get install bonnie++
  bonnie++ -f
  </code></pre>

  <p>We don't care too much about the resulting benchmark numbers here, what we're looking for is to pass without errors. And if you get errors during any of the above &hellip; <em>your computer is not stable.</em></p>

  <p>(I think these disk tests are sufficient for general use, particularly if you consider drives easily RAID-able and replaceable as I do. However, if you want to test your drives more exhaustively, a good resource is the <a href="https://forums.freenas.org/index.php?threads/how-to-hard-drive-burn-in-testing.21451/">FreeNAS "how to burn in hard drives" topic</a>.)</p>

  <h5 id="network">Network</h5>

  <p>I don't have a lot of experience with network hardware failure, to be honest. But I do believe in the cult of bandwidth, and that's one thing we can check.</p>

  <p>You'll need two machines for an <a href="https://en.wikipedia.org/wiki/Iperf">iperf</a> test, which makes it more complex. Here's the server, let's say it's at 10.0.0.1:</p>

  <pre><code>sudo apt-get install iperf
  iperf -s
  </code></pre>

  <p>and here's the client, which will connect to the server and record how fast it can transmit data between the two:</p>

  <pre><code>sudo apt-get install iperf
  iperf -c 10.0.0.1

  ------------------------------------------------------------
  Client connecting to 10.0.0.1, TCP port 5001
  TCP window size: 23.5 KByte (default)
  ------------------------------------------------------------
  [  3] local 10.0.0.2 port 43220 connected with 10.0.0.1 port 5001
  [ ID] Interval       Transfer     Bandwidth
  [  3]  0.0-10.0 sec  1.09 GBytes    933 Mbits/sec
  </code></pre>

  <p>As a point of reference, you should expect to see roughly <strong>120 megabytes/sec (aka 960 megabits)</strong> of real world throughput on a single gigabit ethernet connection. If you're lucky enough to have a 10 gigabit connection, well, good luck reaching that meteoric 1.2 Gigabyte/sec theoretical throughput maximum.</p>

  <h5 id="videocard">Video Card</h5>

  <p>I'm not covering this, because very few of the computers I build these days need more than the stuff built into the CPU to handle video. Which is getting <a href="http://blog.codinghorror.com/the-2016-htpc-build/">surprisingly decent, at last</a>.</p>

  <p>You're a gamer, right? So you'll probably want to boot into Windows and try something like <a href="http://www.ozone3d.net/benchmarks/fur/">furmark</a>. And you <em>should</em> test, because GPUs &ndash; especially gaming GPUs &ndash; are rather cutting edge bits of kit and burn through a lot of watts. Monitor temperatures and system heat, too.</p>

  <p>If you have recommendations for gaming class video card stability testing, share them in the comments.</p>

  <h5 id="ok_maybe_itsstable">OK, <em>Maybe</em> It's Stable</h5>

  <p>This is the regimen I use on the machines I build and touch. And it's worked well for me. I've identified faulty CPUs (once), faulty RAM, faulty disks, and insufficient case airflow early on so that I could deal with them in the lab, before they became liabilities in the field. Doesn't mean they won't fail <em>eventually</em>, but I did all I could to make sure my <s>babies</s> computers can live long and prosper.</p>

  <p>Who knows, with a bit of luck maybe you'll end up like the guy whose <a href="http://arstechnica.com/information-technology/2013/03/epic-uptime-achievement-can-you-beat-16-years/">netware server had <strong>sixteen years</strong> of uptime</a> before it was decommissioned.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/16-years-of-uptime.jpg" alt=""></p>

  <p>These tests are just a starting point. What techniques do you use to ensure the computers you build are stable? How would you improve on these stability tests based on your real world experience?</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] At Stack Overflow, we help developers learn, share, and grow. Whether you’re looking for your next dream job or looking to build out your team, <a href="http://careers.stackoverflow.com" rel="nofollow">we've got your back</a>.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[The Scooter Computer]]></title>
  <description><![CDATA[<p>When we initially deployed our <a href="http://blog.codinghorror.com/building-servers-for-fun-and-prof-ok-maybe-just-for-fun/">handbuilt colocated servers</a> for Discourse in 2013, I needed a way to provide an isolated VPN channel in for secure remote access and troubleshooting. Rather than dedicate a whole server to this task, I purchased <a href="http://blog.codinghorror.com/because-everyone-still-needs-a-router/">the inexpensive, open source firmware friendly Asus RT-N16 router</a>, flashed</p>]]></description>
  <link>https://blog.codinghorror.com/the-scooter-computer/</link>
  <guid isPermaLink="false">53ed9ae1-5020-485e-bbbe-1b5bfc0e07a7</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Wed, 03 Feb 2016 08:52:36 GMT</pubDate>
  <content:encoded><![CDATA[<p>When we initially deployed our <a href="http://blog.codinghorror.com/building-servers-for-fun-and-prof-ok-maybe-just-for-fun/">handbuilt colocated servers</a> for Discourse in 2013, I needed a way to provide an isolated VPN channel in for secure remote access and troubleshooting. Rather than dedicate a whole server to this task, I purchased <a href="http://blog.codinghorror.com/because-everyone-still-needs-a-router/">the inexpensive, open source firmware friendly Asus RT-N16 router</a>, flashed it with the popular TomatoUSB open source firmware, removed the antennas, turned off the WiFi and dropped it off in our colocated rack to let it act as a dedicated VPN access point.</p>

  <p><a href="http://www.amazon.com/dp/B00387G6R8/?tag=codihorr-20"> <br>
  <img alt="Asus RT-N16" title="Asus RT-N16" src="https://blog.codinghorror.com/content/images/uploads/2012/06/6a0120a85dcdae970b016306b1a243970d-800wi.jpg" width="500" height="414"></a></p>

  <p>And that box &ndash; which was $100 then and around $70 now &ndash; worked well enough until now. Although the version of OpenSSL in the 2012 era Tomato firmware we used is not vulnerable to Heartbleed, it's still getting out of date in terms of the encryption it supports and allows. And <a href="http://www.polarcloud.com/tomato">Tomato</a> itself is updated sporadically, chaotically at best.</p>

  <p>Let's face it: <strong>this is just a little box that runs a chopped up version of Linux</strong>, with a bit of specialized wireless hardware and multiple antennas tacked on &hellip; that we're not even using. So when it came time to upgrade, we wondered:</p>

  <blockquote>
  <p>Why not just go with a small box that can run a real, full Linux distro? Wouldn't that be simpler and easier to keep up to date? </p>
  </blockquote>

  <p>After doing some research and asking on Twitter, I discovered there are a ton of amazing little Broadwell "mini-PC" boxes <a href="http://www.aliexpress.com/category/70803003/mini-pcs.html">available on AliExpress</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/IMG_2485.JPG" alt=""></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/IMG_2486.JPG" alt=""></p>

  <p>The specs are kind of amazing for the price. I paid <strong>~$350</strong> each for <a href="http://www.aliexpress.com/item/Fanless-i5-Mini-PC-Windows-Barebone-PC-Broadwell-Intell-Core-i5-5200U-2-7GHz-4K-HTPC/32366202925.html">the ones I selected</a>:</p>

  <ul>
  <li><a href="http://ark.intel.com/products/85212/Intel-Core-i5-5200U-Processor-3M-Cache-up-to-2_70-GHz">i5-5200</a> Broadwell 2 core / 4 thread CPU at 2.2 Ghz - 2.7 Ghz</li>
  <li>16GB DDR3 RAM</li>
  <li>128GB M.2 SSD</li>
  <li>Dual gigabit Realtek 8168 ethernet</li>
  <li>front 4 USB 3.0 ports / rear 4 USB 2.0 ports</li>
  <li>Dual HDMI out</li>
  </ul>

  <p>(There's also optical and analog audio connectors on the front, as well as a SD card reader, which I covered with a sticker since we had no need for audio. I also stripped the WiFi out since we didn't need it, but it was included for the price, too.)</p>

  <p>Selecting the <a href="http://ark.intel.com/products/75990/Intel-Core-i5-4258U-Processor-3M-Cache-up-to-2_90-GHz">i5-4258u</a>, 4GB RAM, and 64GB SSD pushes the price down to <strong>$270</strong>. That's still a solid CPU, only a single generation behind Intel's latest and greatest Skylake, and carrying the midrange i5 moniker; it's no pushover. There are also many, many variants of this box from <a href="http://www.aliexpress.com/category/70803003/mini-pcs.html">other AliExpress sellers</a> that have slightly older, cheaper CPUs that are still plenty powerful. You can easily spec a box similar to this one for $200. </p>

  <p>That's not a whole lot more than the $200 you'd pay for a high end router these days, and as Ars Technica notes, <a href="http://arstechnica.com/gadgets/2016/01/numbers-dont-lie-its-time-to-build-your-own-router/">the average x86 box is radically faster</a>.</p>

  <p><a href="http://arstechnica.com/gadgets/2016/01/numbers-dont-lie-its-time-to-build-your-own-router/"><img src="https://blog.codinghorror.com/content/images/2016/02/homebrew-vs-router-perf-ars.png" alt="" title=""></a></p>

  <p>Note that the above graphs, "homebrew" means an <a href="http://ark.intel.com/products/71995/Intel-Celeron-Processor-1037U-2M-Cache-1_80-GHz">old, 1.8 Ghz Ivy Bridge dual core chip</a>, 3 generations behind current CPUs, that doesn't even merit the i3 or i5 designation, and has no hyperthreading. Do bear that in mind as you keep reading.</p>

  <p><strong>Meet The Scooter Computer</strong></p>

  <p>This box may be small, and only 15 watt TDP, but it is mighty. I spun up a new Digital Ocean droplet and ran a quick benchmark:</p>

  <pre><code>sudo apt-get install sysbench
  sysbench --test=cpu --cpu-max-prime=20000 run
  </code></pre>

  <table>  
  <tr>  
  <td>Tie Shuttle 6<br>  


  <pre>total time:           28.0707s
  total num events:     10000
  total time take:      28.0629
  per-request stats:
  min:             2.77ms
  avg:             2.81ms
  max:             3.99ms
  ~95 percentile:  3.00ms</pre>
  </td>  
  <td>Digital Ocean Droplet<br>  


  <pre>total time:          35.9541s
  total num events:    10000
  total time taken:    35.9492
  per-request stats:
  min:             3.50ms
  avg:             3.59ms
  max:             13.31ms
  ~95 percentile:  3.79ms</pre>
  </td>  
  </tr>  
  </table>

  <p>Results will of course <a href="https://wiki.mikejung.biz/Sysbench#Cloud_.2F_VPS_CPU_Performance_Test_Results_with_Sysbench">vary by cloud provider</a>, but rest assured this box is just as fast as and possibly even faster than the average cloud box you could spin up right now. Of course it is "only" 2 cores / 4 threads, but the more cores you need, the slower they tend to go because of the overall TDP limits of the core package.</p>

  <p>One thing that's not immediately obvious in photos is that this thing is indeed small but <em>hefty</em>, like holding a solid chunk of aluminum in your hand. That's because the box is passively cooled &mdash; the whole case is the heatsink, as the CPU on the bottom of the motherboard mates with the finned top of the case.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/tie-shuttle-top-case.jpg" alt=""></p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/scooter-computer-internals.jpg" alt=""></p>

  <p>Opening this box you realize just how simple things are inside it; it's barely more than a highly integrated motherboard strapped to an aluminum block. This isn't a Steve Jobs truck, a Mac Mini car, or even a motorcycle. <strong>This is <a href="http://blog.codinghorror.com/geek-transportation-systems/">a scooter</a>.</strong></p>

  <blockquote>
  <p>Scooters are very primitive machines; it is both their greatest strength and their greatest weakness. It's arguably the simplest personal wheeled vehicle there is. In these short distance scenarios, scooters tend to win over, say, bicycles because there's less setup and teardown necessary – you don't have to lock up a scooter, nor do you have to wear a helmet. Just hop on and go! You get almost all the benefits of gravity and wheeled efficiency with a minimum of fuss and maintenance. And yes, it's fun, too! </p>
  </blockquote>

  <p>Passively cooled computers are paragons of simplicity and reliable consumer electronics, but passively cooling a "real" x86 PC is the holy grail. To get serious performance you usually need to feed the CPU at least 10 to 20 watts &ndash; and dissipating that kind of energy with zero fans and ambient airflow alone is not trivial. Let's see how our scooter does overnight running <a href="http://www.mersenne.org/download/">Mersenne Primes</a>, which is the heaviest CPU load possible.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/CZ30-0eWYAAZniT.jpg" alt=""></p>

  <p>You can place your hand on the top of the box during this, but it's uncomfortable. And the whole box radiates heat, not just the top. Overall it was completely stable for me during overnight mprime torture testing with the 15w TDP CPU I chose, and I am comfortable with these boxes sitting in our rack in the datacenter, even under extended full load. However, I would be <em>very</em> careful putting a 28w TDP CPU in this box unless you are absolutely sure it won't be at full load very often. Passive cooling is hard.</p>

  <p>Power consumption, as measured by my Kill-a-Watt, ranged from <strong>7 watts</strong> at the Ubuntu Server 14.04 text login screen, to 8-10 watts at an idle Ubuntu 15.10 GUI login screen (the default OS it arrived with), to 14-18 watts in memory testing, to <strong>26 watts in mprime</strong>. </p>

  <p>I should also mention that even under extreme mprime load, both CPUs stayed at 2.5 Ghz indefinitely, which is unusual in my experience. To achieve 2.7 Ghz you need a single threaded load. Considering the base clock of the i5-5200u is 2.2 Ghz, that's quite good! Many 4-6-8 core CPUs drop all the way down to their base clock pretty fast once they have significant load, which makes the "turbo" moniker a bit of a lie.</p>

  <p>(By the way, don't bother using burnP6, it generates way too little heat compared to mprime, which is an absolute <em>monster</em>. If your CPU can survive an overnight run of mprime, I can assure you it's ready for just about anything the real world can throw at it, ever.)</p>

  <p><strong>Disk</strong></p>

  <p>The machine has M.2 slots for two drives, as well as a SATA port and power cable (not pictured, but was included in the box) if you want to mate a 2.5" drive with the drive mounting holes on the bottom of the case. So if you prefer a mirrored two drive RAID array here for reliability, or a giant honking 2TB 2.5" HDD slapped in there for media storage, all of that is possible!</p>

  <p>Be careful, as the internal M.2 slots are <strong>2242</strong>, meaning <a href="http://rog.asus.com/313352014/labels/guides/buying-an-m-2-ssd-how-to-tell-which-is-which/">42mm length</a>. There seem to be mostly (only?) lower cost SSD drives available in this size for whatever reason.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/m-2-length.jpg" alt=""></p>

  <p>Don't worry, though, the bundled 128GB <a href="http://mydigitalssd.com/msata-ssd.php">Phison S9</a> M.2 SSD has <a href="http://www.servethehome.com/mydigitalssd-bp4-128gb-msata-benchmarks-review/">decent performance</a>, roughly equal to a good SSD from a few years ago:</p>

  <pre><code>dd bs=1M count=512 if=/dev/zero of=test conv=fdatasync
  hdparm -Tt /dev/sda

  536870912 bytes (537 MB) copied, 1.52775 s, 351 MB/s
  Timing cached reads:   11434 MB in  2.00 seconds = 5720.61 MB/sec
  Timing buffered disk reads:  760 MB in  3.00 seconds = 253.09 MB/sec
  </code></pre>

  <p>That's respectable SSD performance and won't hold you back in most use cases, but it's not a barn-burning disk subsystem, either. If you want to upgrade, I recommend the <a href="http://www.amazon.com/dp/B00TGIVVKU/?tag=codihorr-20">Samsung 850 EVO</a> which comes in the required form factor.</p>

  <p>Of course the Samsung 850 Pro would fit fine as a traditional 2.5" SATA drive mounted to the case cover, and would perform like this:</p>

  <pre><code>536870912 bytes (537 MB) copied, 1.20895 s, 444 MB/s
  Timing cached reads:   38608 MB in  2.00 seconds = 19330.61 MB/sec
  Timing buffered disk reads: 1584 MB in  3.00 seconds = 527.92 MB/sec
  </code></pre>

  <p><strong>RAM</strong></p>

  <p>Intel limits these Broadwell U class CPUs to 16GB RAM total, so maxing the box out is only going to set you back around $70. Still, that's a significant percentage of the ~$350 total cost, and you may not need that much RAM for what you have in mind.</p>

  <p>However, do be careful that you get dual-channel RAM for lower RAM configurations; you don't want a single 4GB DIMM, you want two 2GB DIMMs. They ship from the vendor with a single DIMM, so beware. It may not matter <a href="http://www.anandtech.com/show/8672/lenovo-thinkstation-p300-workstation-review-haswell-plus-quadro/6">depending on the task</a>, as noted by AnandTech, but our boxes will be used for OpenSSL, and memory is cheap, so why not?</p>

  <p><strong>The Versatile Scooter</strong></p>

  <p>When I began looking at this, I was shocked to discover just how low-end the x86 CPUs are in a lot of "dedicated" devices, such as the official <a href="https://www.pfsense.org/products/">pfSense hardware</a>:</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/pfsense-hardware-lineup.png" alt=""></p>

  <p>Sure, 2.4 Ghz and 8 cores on that C2758 sounds reasonable &ndash; until you realize those are old Intel Bay Trail Atom cores. Even the <em>current</em> Cherry Trail Atom cores aren't so hot. Furthermore, those are probably the maximum "turbo" frequencies being quoted, which are unlikely to be sustained under any kind of real multi-core load. Also, did I mention this is being sold as a $1,400 device? Except for the lack of more than 2 dedicated gigabit ethernet ports, I'd put our scooter computer up against that C2758 any day of the week. And you know what? It'd win.</p>

  <p>I think this logic applies to a lot of dedicated hardware these days &mdash; routers, switches, firewalls, and so on. <strong>You're often better off building up a modern high power, low TDP x86 box and slapping a regular Linux distro on there.</strong></p>

  <p>You can even kinda-sorta fit six of them in a 1U rack space. </p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/02/shuttles-vs-1u.jpg" alt=""></p>

  <p>(Well, except for the power bricks and cables. Vertical mounting on a 1U shelf works out a bit better, and each conveniently came with a stand for vertical operation.)</p>

  <p>Now that I've worked with these boxes, I've become rather enamored of the Scooter Computer concept. Wherever we were thinking that we had to run either:</p>

  <ul>
  <li><p>A virtual machine on big iron for some small but important utility function in our rack.</p></li>
  <li><p>Dedicated, purpose built hardware for networking, firewall, or switching with a custom OS.</p></li>
  </ul>

  <p>&hellip; we can now take advantage of cheap, reliable, flexible, totally solid state commodity x86 hardware that's spread across many machines and running standard Linux distributions, like all the rest of our 1U servers.</p>

  <table>  
  <tr><td class="welovecodinghorror">[advertisement] At Stack Overflow, we put developers first. We already help you find answers to your tough coding questions; now let us help you <a href="http://careers.stackoverflow.com" rel="nofollow">find your next job</a>.</td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Zopfli Optimization: Literally Free Bandwidth]]></title>
  <description><![CDATA[<p>In 2007 I wrote about <a href="http://blog.codinghorror.com/getting-the-most-out-of-png/">using PNGout to produce amazingly small PNG images</a>. I still refer to this topic frequently, as seven years later, the average PNG I encounter on the Internet is very unlikely to be optimized. </p>

  <p>For example, consider <a href="http://pbfcomics.com/274/">this recent Perry Bible Fellowship cartoon</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/01/PBF274-Adam_2-0.png" alt=""></p>

  <p>Saved directly from</p>]]></description>
  <link>https://blog.codinghorror.com/zopfli-optimization-literally-free-bandwidth/</link>
  <guid isPermaLink="false">e055f642-96e3-4eba-918d-eff65bacf2be</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Sat, 02 Jan 2016 09:38:24 GMT</pubDate>
  <content:encoded><![CDATA[<p>In 2007 I wrote about <a href="http://blog.codinghorror.com/getting-the-most-out-of-png/">using PNGout to produce amazingly small PNG images</a>. I still refer to this topic frequently, as seven years later, the average PNG I encounter on the Internet is very unlikely to be optimized. </p>

  <p>For example, consider <a href="http://pbfcomics.com/274/">this recent Perry Bible Fellowship cartoon</a>.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/01/PBF274-Adam_2-0.png" alt=""></p>

  <p>Saved directly from the PBF website, this comic is a 800 &times; 1412, 32-bit color PNG image of 671,012 bytes. Let's save it in a few different formats to get an idea of how much space this image could take up:</p>

  <table style="width:320px">  
  <tr><td>BMP</td><td>24-bit</td><td>3,388,854</td></tr>  
  <tr><td>BMP</td><td>8-bit</td><td>1,130,678</td></tr>  
  <tr><td>GIF</td><td>8-bit, no dither</td><td>147,290</td></tr>  
  <tr><td>GIF</td><td>8-bit, max dither</td><td>283,162</td></tr>  
  <tr><td>PNG</td><td>32-bit</td><td>671,012</td></tr>  
  </table>

  <p>PNG is a win because like GIF, it has built-in compression, but <em>unlike</em> GIF, you aren't limited to cruddy 8-bit, 256 color images. Now what happens when we apply PNGout to this image?</p>

  <table style="width:320px">  
  <tr><td>Default PNG</td><td>671,012</td><td></td></tr>  
  <tr><td>PNGout</td><td>623,859</td><td>7%</td></tr>  
  </table>

  <p>Take any random PNG of unknown provenance, apply PNGout, and you're likely to see around a 10% file size savings, possibly a lot more. Remember, this is <em>lossless</em> compression. The output is identical. It's a smaller file to send over the wire, and the smaller the file, the faster the decompression. This is <strong>free bandwidth</strong>, people! It doesn't get much better than this!</p>

  <p>Except when it does.</p>

  <p>In 2013 Google introduced a new, fully backwards compatible method of compression <a href="http://googledevelopers.blogspot.com/2013/02/compress-data-more-densely-with-zopfli.html">they call Zopfli</a>.</p>

  <blockquote>
  <p>The output generated by Zopfli is typically 3–8% smaller compared to <code>zlib</code> at maximum compression, and we believe that Zopfli represents the state of the art in Deflate-compatible compression. Zopfli is written in C for portability. It is a compression-only library; existing software can decompress the data. Zopfli is bit-stream compatible with compression used in gzip, Zip, PNG, HTTP requests, and others.</p>
  </blockquote>

  <p>I apologize for being super late to this party, but let's test this bold claim. What happens to our PBF comic?</p>

  <table style="width:320px">  
  <tr><td>Default PNG</td><td>671,012</td><td></td></tr>  
  <tr><td>PNGout</td><td>623,859</td><td>7%</td></tr>  
  <tr><td>ZopfliPNG</td><td>585,117</td><td style="color:red">13%</td></tr>  
  </table>

  <p>Looking good. But that's just one image. We're <a href="http://blog.discourse.org/2015/12/emoji-and-discourse/">big fans of Emoji at Discourse</a>, let's try it on the original first release of the <a href="http://emojione.com/">Emoji One</a> emoji set &ndash; that's a complete set of 842 64&times;64 PNG files in 32-bit color:</p>

  <table style="width:320px">  
  <tr><td>Default PNG</td><td>2,328,243</td><td></td></tr>  
  <tr><td>PNGout</td><td>1,969,973</td><td>15%</td></tr>  
  <tr><td>ZopfliPNG</td><td>1,698,322</td><td style="color:red">27%</td></tr>  
  </table>

  <p>Wow. Sign me up for some of that.</p>

  <p>In my testing, Zopfli reliably produces 3 to 8 percent smaller PNG images than even the mighty PNGout, which is an incredible feat. Furthermore, any standard gzip compressed resource can benefit from Zopfli's improved deflate, <a href="https://mathiasbynens.be/demo/jquery-size">such as jQuery</a>:</p>

  <p><img src="https://blog.codinghorror.com/content/images/2016/01/zopfli-vs-gzip.png" alt=""></p>

  <p>Or the standard compression corpus tests:</p>

  <table style="width:360px">  
  <tr><td></td><td><code>gzip -­9</code></td><td>kzip</td><td>Zopfli</td></tr>  
  <tr><td>Alexa­ 10k</td><td>128mb</td><td>125mb</td><td>124mb</td></tr>  
  <tr><td><a href="https://en.wikipedia.org/wiki/Calgary_corpus">Calgary</a></td><td>1017kb</td><td>979kb</td><td>975kb</td></tr>  
  <tr><td><a href="https://en.wikipedia.org/wiki/Canterbury_corpus">Canterbury</a></td><td>731kb</td><td>674kb</td><td>670kb</td></tr>  
  <tr><td><a href="http://mattmahoney.net/dc/textdata">enwik8</a></td><td>36mb</td><td>35mb</td><td>35mb</td></tr>  
  </table>

  <p>(Oddly enough, I had not heard of <a href="http://advsys.net/ken/utils.htm">kzip</a> &ndash; turns out that's our old friend Ken Silverman popping up again, probably using the same compression bag of tricks from his PNGout utility.)</p>

  <p>But there is a catch, because <a href="https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch">there's always a catch</a> &ndash; it's also <strong>80 times slower</strong>. No, that's not a typo. Yes, you read that right.</p>

  <table style="width:300px">  
  <tr><td><code>gzip -­9</code></td><td>5.6s</td></tr>  
  <tr><td><code>7­zip ­mm=Deflate ­mx=9</code></td><td>128s</td></tr>  
  <tr><td>kzip</td><td>336s</td></tr>  
  <tr><td>Zopfli</td><td>454s</td></tr>  
  </table>

  <p>Gzip compression is faster than it looks in the above comparsion, because level 9 is <a href="http://tukaani.org/lzma/benchmarks.html">a bit slow for what it does</a>:</p>

  <table style="width:320px">  
  <tr><td></td><td>Time</td><td>Size</td></tr>  
  <tr><td><code>gzip -1</code></td><td>11.5s</td><td>40.6%</td></tr>  
  <tr><td><code>gzip -2</code></td><td>12.0s</td><td>39.9%</td></tr>  
  <tr><td><code>gzip -3</code></td><td>13.7s</td><td>39.3%</td></tr>  
  <tr><td><code>gzip -4</code></td><td>15.1s</td><td>38.2%</td></tr>  
  <tr><td><code>gzip -5</code></td><td>18.4s</td><td>37.5%</td></tr>  
  <tr><td><code>gzip -6</code></td><td>24.5s</td><td>37.2%</td></tr>  
  <tr><td><code>gzip -7</code></td><td>29.4s</td><td>37.1%</td></tr>  
  <tr><td><code>gzip -8</code></td><td>45.5s</td><td>37.1%</td></tr>  
  <tr><td><code>gzip -9</code></td><td>66.9s</td><td>37.0%</td></tr>  
  </table>

  <p>You decide if that whopping 0.1% compression ratio difference between <code>gzip -7</code>and <code>gzip -9</code> is worth the <em>doubling</em> in CPU time. In related news, this is why pretty much every compression tool's so-called "Ultra" compression level or mode is generally a bad idea. You <a href="http://blog.codinghorror.com/compression-and-cliffs/">fall off an algorithmic cliff</a> pretty fast, so stick with the middle or the optimal part of the curve, which tends to be the default compression level. They do pick those defaults for a reason.</p>

  <p>PNGout was not exactly <em>fast</em> to begin with, so imagining something that's 80 times slower (at best!) to compress an image or a file is definite cause for concern. You may not notice on small images, but try running either on a larger PNG and it's basically time to go get a sandwich. Or if you have a multi-core CPU, 4 to 16 sandwiches. This is why applying Zopfli to user-uploaded images might not be the greatest idea, because the first server to try Zopfli-ing a 10k &times; 10k PNG image is in for a hell of a surprise.</p>

  <p>However, remember that <em>decompression</em> is still the same speed, and totally safe. This means <strong>you probably only want to use Zopfli on pre-compiled resources</strong>, which are designed to be compressed once and downloaded millions of times &ndash; rather than a bunch of PNG images your users uploaded which may only be viewed a few hundred or thousand times at best, regardless of how optimized the images happen to be.</p>

  <p>For example, at <a href="http://discourse.org">Discourse</a> we have a default avatar renderer which produces nice looking PNG avatars for users based on the first letter of their username, plus a color scheme selected via the hash of their username. Oh yes, and the very nice <a href="https://www.google.com/fonts/specimen/Roboto">Roboto open source font</a> from Google. </p>

  <table style="width:320px">  
  <tr>  
  <td><img src="https://blog.codinghorror.com/content/images/2016/01/discourse-default-avatar-a.png" style="border-radius:50%" width="100">  
  </td><td><img src="https://blog.codinghorror.com/content/images/2016/01/discourse-default-avatar-d.png" style="border-radius:50%" width="100">  
  </td><td><img src="https://blog.codinghorror.com/content/images/2016/01/discourse-default-avatar-s.png" style="border-radius:50%" width="100">  
  </td></tr>  
  </table>

  <p>We spent a <em>lot</em> of time optimizing the output avatar images, because these avatars can be served millions of times, and pre-rendering the whole lot of them, given the constraints of &hellip;</p>

  <ul>
  <li>10 numbers</li>
  <li>26 letters</li>
  <li>~250 color schemes</li>
  <li>~5 sizes</li>
  </ul>

  <p>&hellip; isn't unreasonable at around 45,000 unique files. We also have a centralized https CDN we set up to to serve avatars (if desired) across all Discourse instances, to further reduce load and increase cache hits.</p>

  <p>Because these images stick to shades of one color, I reduced the color palette to 8-bit (actually 128 colors) to save space, and of course we run PNGout on the resulting files. They're about as tiny as you can get. When I ran Zopfli on the above avatars, I was super excited to see my expected 3 to 8 percent free file size reduction and after the console commands ran, I saw that saved &hellip; 1 byte, 5 bytes, and 2 bytes respectively. <a href="https://wompwompwomp.com/">Cue sad trombone</a>.</p>

  <p>(Yes, it is technically possible <a href="http://pointlessramblings.com/posts/pngquant_vs_pngcrush_vs_optipng_vs_pngnq/">to produce strange "lossy" PNG images</a>, but I think that's counter to the spirit of PNG which is designed for <em>lossless</em> images. If you want lossy images, <a href="http://blog.codinghorror.com/screenshots-jpeg-vs-gif/">go with JPG</a> or another lossy format.)</p>

  <p>The great thing about Zopfli is that, assuming you are OK with the extreme up front CPU demands, it is a "set it and forget it" optimization step that can apply anywhere and will never hurt you. Well, other than possibly burning a lot of spare CPU cycles.</p>

  <p>If you work on a project that serves compressed assets, take a close look at Zopfli. It's not a silver bullet &ndash; as with all advice, run the tests on <em>your</em> files and see &ndash; but it's about as close as it gets to <strong>literally free bandwidth</strong> in our line of work.</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] <a href="http://careers.stackoverflow.com" rel="nofollow">Find a better job the Stack Overflow way</a> - what you need when you need it, no spam, and no scams.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[The Hugging Will Continue Until Morale Improves]]></title>
  <description><![CDATA[<p>I saw in today's news that Apple <a href="https://t.co/KpC9xID5kU">open sourced their Swift language</a>. One of the most influential companies in the world explicitly adopting an open source model &ndash; that's great! I'm a believer. One of the big reasons <a href="http://blog.codinghorror.com/civilized-discourse-construction-kit/">we founded Discourse</a> was to build an open source solution that anyone,</p>]]></description>
  <link>https://blog.codinghorror.com/the-hugging-will-continue-until-morale-improves/</link>
  <guid isPermaLink="false">358ab343-1b4b-496d-a4b0-d1d122690d47</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Fri, 04 Dec 2015 11:17:05 GMT</pubDate>
  <content:encoded><![CDATA[<p>I saw in today's news that Apple <a href="https://t.co/KpC9xID5kU">open sourced their Swift language</a>. One of the most influential companies in the world explicitly adopting an open source model &ndash; that's great! I'm a believer. One of the big reasons <a href="http://blog.codinghorror.com/civilized-discourse-construction-kit/">we founded Discourse</a> was to build an open source solution that anyone, anywhere could use and safely build upon.</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">It&#39;s not that Unix won -- just that closed source lost. Big time.</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/616377394253795328">July 1, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>People were also encouraged that Apple was so refreshingly open about this whole process and involving the larger community in the process. They even <a href="https://twitter.com/mcdado/status/672509114476601345">hired from the community</a>, which is something I always urge companies to do.</p>

  <p>Also, not many people were, shall we say &hellip; <em>fans</em> &hellip; of Objective C <a href="http://www.antonzherdev.com/post/70064588471/top-13-worst-things-about-objective-c">as a language</a>. There was a lot of community interest in having another viable modern language to write iOS apps in, and to Apple's credit, they produced Swift, and even promised to open source it by the end of the year. And they delivered, in a deliberate, thoughtful way. (Did I mention that <a href="https://github.com/apple/swift-cmark">they use CommonMark</a>? That's kind of awesome, too.)</p>

  <p>One of my heroes, Miguel de Icaza, happens to have <em>lots</em> of life experience in open sourcing things that were not exactly open source to start with. He applauded the move, and even made a small change to his Mono project in tribute:</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">When Swift was open sourced today, I saw they had a Code of Conduct. We had to follow suit, Mono has adopted it: <a href="https://t.co/hVO3KL1Dn5">https://t.co/hVO3KL1Dn5</a></p>&mdash; Miguel de Icaza (@migueldeicaza) <a href="https://twitter.com/migueldeicaza/status/672590341757927426">December 4, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>Which I also thought was kinda cool.</p>

  <p>It surprises me that anyone could ever object to the mere <em>presence</em> of a code of conduct. But <a href="https://medium.com/@jmspool/safe-conferences-are-deliberately-designed-2849b6cd3658">some people do</a>.</p>

  <blockquote>
  <ul>
  <li><p>A weak Code of Conduct is a placebo label saying a conference is safe, without actually ensuring it’s safe.</p></li>
  <li><p>Absence of a Code of Conduct does not mean that the organizers will provide an unsafe conference.</p></li>
  <li><p>Creating safety is not the same as creating a feeling of safety.</p></li>
  <li><p>Things organizers can do to make events safer: Restructure parties to reduce unsafe intoxication-induced behavior; work with speakers in advance to minimize potentially offensive material; and provide very attentive, mindful customer service consistently through the attendee experience.</p></li>
  <li><p>Creating a safe conference is more expensive than just publishing a Code of Conduct to the event, but has a better chance of making the event safe.</p></li>
  <li><p>Safe conferences are the outcome of a deliberate design effort.</p></li>
  </ul>
  </blockquote>

  <p>I have to say, I don't understand this at all. Even if you do believe these things, why would you say them <em>out loud?</em> What possible constructive outcome could result from you saying them? It's a textbook case of honesty <a href="http://blog.codinghorror.com/trust-me-im-lying/">not always being the best policy</a>. If this is all you've got, just say nothing, or wave people off with platitudes, like politicians do. And if you're Jared Spool, notable and <a href="https://en.wikipedia.org/wiki/Jared_Spool">famous within your field</a>, it's even worse &ndash; what does this say to everyone else working in your field?</p>

  <p>Mr. Spool's central premise is this:</p>

  <blockquote>
  <p>Creating safety is not the same as creating a feeling of safety.</p>
  </blockquote>

  <p>Which, actually &hellip; isn't true, and runs counter to everything I know about empathy. If you've ever watched It's Not About the Nail, you'll understand that <strong>a <em>feeling</em> of safety is, in fact, what many people are looking for</strong>. It's not the whole story by any means, but it's a very important starting point. An anchor.</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/-4EDhdAHrOg" frameborder="0" allowfullscreen></iframe>

  <p>People understand <a href="https://medium.com/@ag_dubs/no-true-conference-organizer-dd0ff11294a">you cannot possibly protect them from every single possible negative outcome at a conference</a>. That's absurd. But they also want to hear you stand up for them, and say out loud that, yes, these are the things we believe in. This is what we know to be true. Here is how we will look out for each other.</p>

  <p>I also had a direct flashback to Deborah Tannen's groundbreaking <a href="http://www.amazon.com/dp/0060959622/?tag=codihorr-20">You Just Don't Understand</a>, in which you learn that <strong>men are all about fixing the problem</strong>, so much so that they rush headlong into any remotely plausible solution, without stopping along the way to actually <em>listen</em> and appreciate the depth of the problem, which maybe &hellip; can't really even <em>be</em> fixed?</p>

  <blockquote>
  <p>If women are often frustrated because men do not respond to their troubles by offering matching troubles, men are often frustrated because women do &hellip; he feels she is trying to take something away from him by denying the uniqueness of his experience &hellip; if women resent men's tendency to offer solutions to problems, men complain about women's refusal to take action to solve the problems they complain about.</p>
  
  <p>Since many men see themselves as problem solvers, a complaint or a trouble is a challenge &hellip; Trying to solve a problem or fix a trouble focuses on the message level. But for most women who habitually report problems at work or in friendships, the message is not the main point &hellip; trouble talk is intended to reinforce rapport by sending the metamessage "We're the same; you're not alone."</p>
  
  <p>Women are frustrated when they not only don’t get this reinforcement but, quite the opposite, feel distanced by the advice, which seems to send the metamessage "We’re not the same. You have the problems; I have the solutions."</p>
  </blockquote>

  <p>Having children really underscored this point for me. The quickest way to turn a child's frustration into a screaming, explosive tantrum is to <strong>try to fix their problem for them</strong>. This is such a hard thing for engineers to wrap their heads around, particularly male engineers, because we are <em>all about</em> fixing the problems. That's what we do, right? That's why we exist? We fix problems? </p>

  <p>I once wrote this in reply to <a href="https://community.imgur.com">an Imgur discussion topic</a> about navigating an "emotionally charged sitation":</p>

  <blockquote>
  <p>Oh, you want a master class in dealing with emotionally charged situations? Well, why didn't you just say so?</p>
  
  <p><strong>Have kids.</strong> Within a few years you will learn to be an expert in dealing with this kind of stuff, because what nobody tells you about having kids is that for the first ~5 years, they are constantly. freaking. the. f**k. out.</p>
  
  <p><a href="http://jasongood.net/365/2012/12/46-reasons-why-my-three-year-old-might-be-freaking-out/">46 Reasons My Three Year Old Might Be Freaking Out</a></p>
  
  <p>If this seems weird to you, or like some kind of made up exaggerated hilarious absurd brand of humor, oh trust me. It's not. Real talk. <em>This is actually how it is.</em></p>
  
  <p>In their defense, it's not their fault: they've never felt fear, anger, hunger, jealousy, love, or any of the dozen other incredibly complex emotions you and I deal with on a daily basis. So they learn. But along the way, there will be many many many manymanymanymany freakouts. And guess who's there to help them navigate said freakouts?</p>
  
  <p>You are.</p>
  
  <p>What works <a href="http://blog.codinghorror.com/how-to-talk-to-human-beings/">is surprisingly simple</a>:</p>
  
  <ul>
  <li>Be there.</li>
  <li>Listen.</li>
  <li>Empathize, hug, and echo back to them. <em>Don't</em> try to solve their problems! DO NOT DO IT! Paradoxically, this only makes it way worse if you do. Let them work through the problem on their own. They always will &ndash; and knowing someone trusts you enough to figure our your own problems is a major psychological boost.</li>
  </ul>
  
  <p>You gotta <a href="http://learn.genetics.utah.edu/content/epigenetics/rats/">lick your rats</a>, man.</p>
  
  <p>(protip: this works identically on adults and kids. Turns out most so-called adults aren't fully grown up. Who knew?)</p>
  </blockquote>

  <p>I guess my point is that rats aren't so different from people. We all want the same thing. Comfort from someone who can tell us that the world is safe, the world is not out to get you, that bad things can (and might) happen to you but <em>you'll still be OK because we will help you</em>. We're all in this thing together, you're a human being much like myself and we love you. </p>

  <p><strong>That's why a visible, public code of conduct is a good idea, not only at an in-person conference, but also on a software project like Swift, or Mono.</strong> But programmers being programmers &ndash; because they spend all day every day mired in the crazy world of infinitely recursive rules from their OS, from their programming language, from their APIs, from their tools &ndash; are rules lawyers <em>par excellence</em>. Nobody on planet Earth is better at arguing to the death over a set of completely arbitrary, made up rules than the average programmer.</p>

  <p>I knew in my heart of hearts that someone &ndash; and by someone I mean a programmer &ndash; would inevitably complain about the fact that Mono had added a code of conduct, another "unnecessary" ruleset. So I made a programmer joke.</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/migueldeicaza">@migueldeicaza</a> I find these rules offensive and will be fining a complaint</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/672604329627418630">December 4, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>This is the second time in as many days that I made what I <em>thought</em> was an obvious joke on Twitter that was interpreted seriously.</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">When someone starts at Discourse, I have the talk with them. &quot;You remember your family? Forget them. Look at me. *We* are your family now.&quot;</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/672142544642248704">December 2, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>OK, maybe sometimes my Twitter jokes aren't very good. Well, you know, that's just, like &hellip; <em>your opinion</em>, man. I should probably switch from Twitter to Myspace or Ello or Google Plus or Snapchat or something.</p>

  <p>But it bothered me that people, any people, would think I actually asked new hires to put the company above their family.* Or that I didn't believe in a code of conduct. I guess some of that comes from having ~200k followers; once your audience gets big enough, <a href="https://en.wikipedia.org/wiki/Poe%27s_law">Poe's Law</a> becomes inevitable?</p>

  <p>Anyway, I wanted to say I'm sorry. And I'm particularly sorry that <a href="http://eev.ee/">eevee</a>, who wrote that <em>awesome</em> <a href="http://blog.codinghorror.com/the-php-singularity/">PHP is a Fractal of Bad Design article that I once riffed on</a>, thought I was serious, or even worse, that my joke was in bad taste. Even though <a href="http://eev.ee/blog/2015/09/17/the-sad-state-of-web-app-deployment/">the negative article about Discourse</a> eevee wrote did kinda hurt my feelings.</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/samsaffron">@samsaffron</a> <a href="https://twitter.com/JakubJirutka">@JakubJirutka</a> programmers should not have feelings that is a liability</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/649743704069029888">October 2, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>I know we have our differences, but if we as programmers can't come together through our collective shared horror over PHP, the Nickelback of programming languages, then clearly I have failed.</p>

  <p>To show that <strong>I absolutely do believe in the value of a code of conduct</strong>, even as public statements of intent that we may not completely live up to, even if we've never had any incidents or problems that would require formal statements &ndash; I'm also adding a code of conduct as defined by <a href="http://contributor-covenant.org/">contributor-covenant.org</a> to the <a href="https://github.com/discourse/discourse">Discourse project</a>. We're all in this open source thing together, you're a human being <a href="http://blog.codinghorror.com/what-if-we-could-weaponize-empathy/">very much like us</a>, and we vow to treat you with the same respect we'd want you to treat us. This should not be controversial. It should be common. And saying so matters.</p>

  <p>If you maintain an open source project, I strongly urge you to consider formally adopting a <a href="http://contributor-covenant.org/">code of conduct</a>, too.</p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/codinghorror">@codinghorror</a> hugs!</p>&mdash; Miguel de Icaza (@migueldeicaza) <a href="https://twitter.com/migueldeicaza/status/672619657703084033">December 4, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>The hugging will continue until morale improves.</p>

  <p><small>*  That's only required of co-founders</small></p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] Building out your tech team? <a href="http://careers.stackoverflow.com/products" rel="nofollow">Stack Overflow Careers</a> helps you hire from the largest community for programmers on the planet. We built our site with developers like you in mind.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[The 2016 HTPC Build]]></title>
  <description><![CDATA[<p>I've <a href="http://blog.codinghorror.com/if-loving-computers-is-wrong-i-dont-want-to-be-right/">loved many computers in my life</a>, but the HTPC has always had a special place in my heart. It's the only always-on workhorse computer in our house, it is utterly silent, totally reliable, sips power, and it's at the center of our home entertainment, networking, storage, and gaming. This</p>]]></description>
  <link>https://blog.codinghorror.com/the-2016-htpc-build/</link>
  <guid isPermaLink="false">6fe27ca7-a24e-40be-9ff3-c9a1b8feb28b</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Mon, 30 Nov 2015 06:42:33 GMT</pubDate>
  <content:encoded><![CDATA[<p>I've <a href="http://blog.codinghorror.com/if-loving-computers-is-wrong-i-dont-want-to-be-right/">loved many computers in my life</a>, but the HTPC has always had a special place in my heart. It's the only always-on workhorse computer in our house, it is utterly silent, totally reliable, sips power, and it's at the center of our home entertainment, networking, storage, and gaming. This handy box does it all, 24/7.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/antec-itx-casejpg.jpg" alt=""></p>

  <p>I love this little machine to death; it's always been there for me and my family. The <b>steady march of improvements in my HTPC build</b> over the years lets me look back and see how far the old beige box PC has come in the decade I've been blogging:</p>

  <table cellpadding="4" cellspacing="4">  
  <tr>  
  <td><a href="http://blog.codinghorror.com/pentium-m-home-theater-pc/">2005</a></td><td>~$1000</td><td>512MB RAM, 1 CPU</td><td>80w  
  </td></tr>  
  <tr>  
  <td><a href="http://blog.codinghorror.com/building-your-own-home-theater-pc/">2008</a></td><td>~$520</td><td>2GB RAM, 2 CPU</td><td>45w  
  </td></tr>  
  <tr>  
  <td><a href="http://blog.codinghorror.com/revisiting-the-home-theater-pc/">2011</a></td><td>~$420</td><td>4GB RAM, 2/4 CPU + GPU</td><td>22w  
  </td></tr>  
  <tr>  
  <td><a href="http://blog.codinghorror.com/the-2013-htpc-build/">2013</a></td><td>~$300</td><td>8GB RAM, 2/4 CPU + GPU&times;2</td><td>15w  
  </td></tr>  
  <tr>  
  <td><b>2016</b></td><td><b>~$320</b></td><td><b>8GB RAM, 2/4 CPU + GPU&times;4</b></td><td><font color="red"><b>10w</b></font>  
  </td></tr>  
  </table>

  <p>As expected, the per-thread performance increase from 2013's Haswell CPU to 2016's Skylake CPU is modest &ndash; 20 percent at best, and that might be rounding up. About all you can do is slap more cores in there, to very limited benefit in most applications. The 6100T I chose is dual-core plus hyperthreading, which I consider the sweet spot, but there are some other <a href="http://ark.intel.com/compare/88200,88183,90734,90725">Skylake 6000 series variants at the same 35w power envelope</a> which offer true quad-core, or quad-core plus hyperthreading &ndash; and, inevitably, a slightly lower base clock rate. So it goes.</p>

  <p>The real story is how idle power consumption was <strong>reduced another 33 percent</strong>. Here's what I measured with <a href="http://blog.codinghorror.com/why-estimate-when-you-can-measure/">my trusty kill-a-watt</a>:</p>

  <ul>  
  <li><b>10w</b> idle with display off  
  </li><li>11w idle with display on  
  </li><li>13w active standard netflix (720p?) movie playback  
  </li><li>14w multiple torrents, display off  
  </li><li>15w 1080p video playback in MPC-HC x64  
  </li><li>40w Lego Batman 3 high detail 720p gameplay  
  </li><li><b>56w</b> Prime95 full CPU load + Rthdribl full GPU load  
  </li></ul>

  <p>These are impressive numbers, much better than I expected. Maybe part of it is the latest Windows 10 update which <a href="http://www.anandtech.com/show/9751/examining-intel-skylake-speed-shift-more-responsive-processors">supports the new Speed Shift technology in Skylake</a>. Speed Shift hands over CPU clockspeed control to the CPU itself, so it can ramp its internal clock up and down dramatically faster than the OS could. A Skylake CPU, with the right OS support, gets up to speed and back to idle faster, resulting in better performance and less overall power draw.</p>

  <p>Skylake's on-board <b>HD 530 graphics is about twice as fast as the HD 4400 that it replaces</b>. Haswell offered the first reasonable big screen gaming GPU on an Intel CPU, but only just. 720p was <em>mostly</em> attainable in older games with the HD 4400, but I sometimes had to drop to medium detail settings, or lower. Two generations on, with the HD 530, even recent games like GRID Autosport, Lego Jurassic Park and so on can now be played at 720p with high detail settings at consistently high framerates. It depends on the game, but a few can even be played at 1080p now with medium settings. I did have at least one saved benchmark result on the disk to compare with:</p>

  <table>  
  <tr>  
  <td>GRID 2, 1280&times;720, high detail defaults</td><td></td><td></td><td>  
  </td></tr>  
  <tr>  
  <td></td><td>Max</td><td>Min</td><td>Avg  
  </td></tr>  
  <tr>  
  <td>i3-4130T, Intel HD 4400 GPU</td><td>32</td><td>21</td><td>27  
  </td></tr>  
  <tr>  
  <td>i3-6100T, Intel HD 530 GPU</td><td>50</td><td>32</td><td>39  
  </td></tr>  
  </table>

  <p>Skylake is a legitimate gaming system on a chip, provided you are OK with 720p. It's tremendous fun to play Lego Batman 3 with my son.</p>

  <iframe width="560" height="315" src="https://www.youtube.com/embed/7bsHsp2WXUI" frameborder="0" allowfullscreen></iframe>

  <p>At 720p using high detail settings, where there used to be many instances of notable slowdown, particularly in co-op, it now feels very smooth throughout. And since games are much cheaper on PC than consoles, <a href="http://store.steampowered.com/">particularly through Steam</a>, we have access to a complete range of gaming options from new to old, from indie to mainstream &ndash; and an enormous, inexpensive back catalog.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/steam-game-choices.jpg" alt=""></p>

  <p>Of course, this is still far from the performance you'd get out of a $300 video card or a $300 console. You'll never be able to play a cutting edge, high end game like GTA V or Witcher 3 on this HTPC box. But <em>you may not need to</em>. <a href="http://store.steampowered.com/streaming/">Steam in-home streaming</a> has truly come into its own in the last year. I tried streaming Batman: Arkham Knight from my beefy home office computer to the HTPC at 1080p, and I was surprised to discover just how effortless it was &ndash; nor could I detect any visual artifacts or input latency.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/steam-streaming.jpg" alt=""></p>

  <p>It's <a href="http://www.pcgamer.com/how-to-set-up-steam-in-home-streaming-on-your-pc/#page-1">super easy to set up</a> &ndash; just have the Steam client running on both machines at a logged in Windows desktop (can't be on the lock screen), and press the Stream button on any game that you don't have installed locally. Be careful with WiFi when streaming high resolutions, obviously, but if you're on a wired network, I found the experience is nearly identical to playing the game locally. As long as the game has native console / controller support, like Arkham Knight and Fallout 4, streaming to the big screen works great. Try it! That's how Henry and I are going to play through <a href="http://store.steampowered.com/app/225540/">Just Cause 3</a> this Tuesday and <a href="http://blog.codinghorror.com/my-holiday-in-beautiful-panau/">I can't wait</a>.</p>

  <p>As before in 2013, I only upgraded the guts of the system, so the incremental cost is low.</p>

  <ul>
  <li><a href="http://www.amazon.com/dp/B015CQ8D9Q/?tag=codihorr-20">GA-H170N-WIFI</a> H170 motherboard &mdash; $120</li>
  <li><a href="http://www.amazon.com/dp/B00MMLUZ2I/?tag=codihorr-20">8GB DDR4</a> RAM &mdash; $46</li>
  <li><a href="http://www.amazon.com/dp/B0161V02ZO/?tag=codihorr-20">Intel i3-6100T</a> 35w, 3.2 GHz dual core CPU &mdash; $155</li>
  </ul>

  <p>That's a total of <strong>$321</strong> for this upgrade cycle, about the cost of a new Xbox One or PS4. The i3-6100T should be a bit cheaper; according to Intel it has the same list price as the i3-6100, but suffers from weak availability. The motherboard I chose is a little more expensive, too, perhaps because it <a href="http://www.gigabyte.com/products/product-page.aspx?pid=5552#ov">includes extras like built in WiFi and M.2 support</a>, although I'm not using either quite yet. You might be able to source a cheaper H170 motherboard than mine.</p>

  <p>The rest of the system has <a href="http://blog.codinghorror.com/the-2013-htpc-build/">not changed much since 2013</a>:</p>

  <ul>
  <li><a href="http://www.amazon.com/gp/product/B0035UETHW/?tag=codihorr-20">PicoPSU 90</a> &mdash; $50</li>
  <li><a href="http://www.amazon.com/gp/product/B0035FIS2O/?tag=codihorr-20">Antec ISK 300-150</a> &mdash; $68</li>
  <li><a href="http://www.amazon.com/dp/B00OBRE5UE/?tag=codihorr-20">512GB SSD boot drive</a> &mdash; $150</li>
  <li><a href="http://www.amazon.com/dp/B00I8O6OQ4/?tag=codihorr-20">2TB 2.5" HDD</a> &times; 2 &mdash; $200</li>
  </ul>

  <p>Populate these items to taste, pick whatever drives and mini-ITX case you prefer, but <strong>definitely stick with the PicoPSU</strong>, because removing the large, traditional case power supply makes the setup both a) much more power efficient at low wattage, and b) much roomier inside the case and easier to install, upgrade, and maintain.</p>

  <p>I also switched to <a href="http://www.amazon.com/dp/B015IX3X3E/?tag=codihorr-20">Xbox One controllers</a>, for no really good reason other than the Xbox 360 is getting more obsolete every month, and now that my beloved Rock Band 4 is available on next-gen systems, I'm trying to slowly evict the 360s from my house.</p>

  <p><a href="http://www.amazon.com/dp/B015IX3X3E/?tag=codihorr-20"><img src="https://blog.codinghorror.com/content/images/2015/11/xbox-one-controller.jpg" alt="" title=""></a></p>

  <p>The <a href="http://www.amazon.com/dp/B00ZB7W4QU/?tag=codihorr-20">Windows 10 wireless Xbox One adapter</a> does have some perks. In addition to working with the newer and slightly nicer gamepads from the Xbox One, it supports an audio stream over each controller via the controller's headset connector. But really, for the purposes of Steam gaming, any USB controller will do.</p>

  <p>While I've been over the moon in love with my HTPC for years, and I liked the Xbox 360, I have been thoroughly unimpressed with my newly purchased Xbox One. Both the new and old UIs are hard to use, it's quite slow relative to my very snappy HTPC, and it has a ton of useless features that I don't care about, like broadcast TV support. About all the Xbox One lets you do is <em>sometimes</em> play next gen games at 1080p without paying $200 or $300 for a fancy video card, and let's face it &ndash; the PS4 does that slightly better. <strong>If those same games are available on PC, you'll have a better experience streaming them from a gaming PC</strong> to either a cheap Steam streaming box, or a generalist HTPC like this one. </p>

  <p>The Xbox One and PS4 are <a href="http://www.extremetech.com/gaming/156273-xbox-720-vs-ps4-vs-pc-how-the-hardware-specs-compare">effectively plain old PCs</a>, built on:</p>

  <ul>
  <li>Intel Atom class (aka slow) AMD 8-core x86 CPU</li>
  <li>8 GB RAM</li>
  <li>AMD Radeon 77xx / 78xx GPUs</li>
  <li>cheap commodity 512GB or 1TB hard drives (not SSDs)</li>
  </ul>

  <p>The <strong>golden age of x86 gaming</strong> is well upon us. That's why the future of PC gaming is looking brighter every day. We can see it coming true in the solid GPU and idle power improvements in Skylake, riding the inevitable wave of x86 becoming the dominant kind of (non mobile, anyway) gaming for the forseeable future. </p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] At Stack Overflow, we help developers learn, share, and grow. Whether you’re looking for your next dream job or looking to build out your team, <a href="http://careers.stackoverflow.com" rel="nofollow">we've got your back</a>.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[To ECC or Not To ECC]]></title>
  <description><![CDATA[<p>On one of my visits to the <a href="http://www.computerhistory.org/">Computer History Museum</a> &ndash; and by the way this is an absolute <em>must-visit</em> place if you are ever in the San Francisco bay area &ndash; I saw an early Google server rack circa 1999 in the exhibits.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/old-google-server-rack.jpg" alt=""></p>

  <p>Not too fancy, right? Maybe even</p>]]></description>
  <link>https://blog.codinghorror.com/to-ecc-or-not-to-ecc/</link>
  <guid isPermaLink="false">5f900a66-ee1b-4184-9add-26c445923c59</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Thu, 19 Nov 2015 23:44:21 GMT</pubDate>
  <content:encoded><![CDATA[<p>On one of my visits to the <a href="http://www.computerhistory.org/">Computer History Museum</a> &ndash; and by the way this is an absolute <em>must-visit</em> place if you are ever in the San Francisco bay area &ndash; I saw an early Google server rack circa 1999 in the exhibits.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/old-google-server-rack.jpg" alt=""></p>

  <p>Not too fancy, right? Maybe even &hellip; a little janky? This is <a href="http://blog.codinghorror.com/building-a-computer-the-google-way/">building a computer the Google way</a>:</p>

  <blockquote>
  <p>Instead of buying whatever pre-built rack-mount servers Dell, Compaq, and IBM were selling at the time, <strong>Google opted to hand-build their server infrastructure themselves</strong>. The sagging motherboards and hard drives are literally propped in place on handmade plywood platforms. The power switches are crudely mounted in front, the network cables draped along each side. The poorly routed power connectors snake their way back to generic PC power supplies in the rear.</p>
  
  <p>Some people might look at these early Google servers and see an amateurish fire hazard. Not me. I see a prescient understanding of how inexpensive commodity hardware would shape today's internet. I felt right at home when I saw this server; it's exactly what I would have done in the same circumstances. This rack is a perfect example of the commodity x86 market D.I.Y. ethic at work: if you want it done right, and done inexpensively, you build it yourself.</p>
  </blockquote>

  <p>This rack is now immortalized in <a href="http://americanhistory.si.edu/press/fact-sheets/google-corkboard-server-1999">the National Museum of American History</a>. Urs Hölzle <a href="https://plus.google.com/+UrsH%C3%B6lzle/posts/VGwMnY3oUSY">posted lots more juicy behind the scenes details</a>, including the exact specifications:</p>

  <ul>
  <li>Supermicro P6SMB motherboard</li>
  <li>256MB PC100 memory</li>
  <li>Pentium II 400 CPU</li>
  <li>IBM Deskstar 22GB hard drives (&times;2)</li>
  <li>Intel 10/100 network card</li>
  </ul>

  <p>When I <a href="http://blog.codinghorror.com/farewell-stack-exchange/">left Stack Exchange</a> (sorry, <a href="https://blog.stackoverflow.com/2015/09/were-changing-our-name-back-to-stack-overflow/">Stack Overflow</a>) one of the things that excited me most was <strong>embarking on a new project using 100% open source tools.</strong> That project is, of course, <a href="http://discourse.org">Discourse</a>.</p>

  <p>Inspired by Google and their use of cheap, commodity x86 hardware to scale on top of the open source Linux OS, I also <a href="http://blog.codinghorror.com/building-servers-for-fun-and-prof-ok-maybe-just-for-fun/">built our own servers</a>.  When I get stressed out, when I feel the world weighing heavy on my shoulders and I don't know where to turn &hellip; <em>I build servers</em>. It's therapeutic. </p>

  <blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">I like to give servers a little pep talk while I build them. &quot;Who&#39;s the best server! Who&#39;s the fastest server!&quot;</p>&mdash; Jeff Atwood (@codinghorror) <a href="https://twitter.com/codinghorror/status/666062934171189249">November 16, 2015</a></blockquote>  

  <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

  <p>Don't judge me, man.</p>

  <p>But more seriously, with the release of Intel's latest Skylake architecture, it's finally time to upgrade our 2013 era Discourse servers to the latest and greatest, something reflective of 2016 &ndash; which means building even more servers.</p>

  <p>Discourse <a href="http://blog.codinghorror.com/why-ruby/">runs on a Ruby stack</a> and one thing we learned early on is that <strong>Ruby demands exceptional single threaded performance</strong>, aka, a CPU running as fast as possible. Throwing umptazillion CPU cores at Ruby doesn't buy you a whole lot other than being able to handle more requests at the same time. Which is nice, but doesn't get you <em>speed</em> per se. Someone made a helpful technical video to illustrate exactly how this all works:</p>

  <video poster="/content/images/2015/11/javascript-python-ruby-apps.jpg" width="100%" preload="none" controls><source src="http://discourse-cdn.codinghorror.com/uploads/default/original/3X/1/0/1049b1846f0cfa65ed0a9b4ab970d57d6dc0bd5a.mp4"></video>

  <p>This is by no means exclusive to Ruby; other languages like JavaScript and Python also share this trait. And Discourse itself is a JavaScript application delivered through the browser, which exercises the mobile / laptop / desktop client CPU. Mobile devices reaching near-parity with desktop performance in single threaded performance is something we're betting on in a big way with Discourse.</p>

  <p>So, good news! Although PC performance has been <a href="http://blog.codinghorror.com/the-pc-is-over/">incremental at best in the last 5 years</a>, between Haswell and Skylake, Intel managed to deliver a respectable per-thread performance bump. Since we are upgrading our servers from Ivy Bridge (very similar to the i7-3770k), the generation before Haswell, I'd <a href="http://www.anandtech.com/show/9483/intel-skylake-review-6700k-6600k-ddr4-ddr3-ipc-6th-generation/11">expect</a> a solid 33% performance improvement at minimum.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/skylake-javascript-performance-boost-1.png" alt=""></p>

  <p>Even worse, the more cores they pack on a single chip, the slower they all go. From Intel's current Xeon E5 lineup:</p>

  <ul>
  <li>E5-1680 &rarr; 8 cores, 3.2 Ghz</li>
  <li>E5-1650 &rarr; 6 cores, 3.5 Ghz</li>
  <li>E5-1630 &rarr; 4 cores, 3.7 Ghz</li>
  </ul>

  <p>Sad, isn't it? Which brings me to the following build for our core web tiers, which optimizes for "lots of inexpensive, fast boxes"</p>

  <table width="100%">  
  <tr>  
  <td><b>2013</b></td>  
  <td><b>2016</b></td>  
  </tr>  
  <tr>  
  <td valign="top">  
  Xeon E3-1280 V2 Ivy Bridge 3.6 Ghz / 4.0 Ghz quad-core ($640)<br>  
  SuperMicro X9SCM-F-O mobo ($190)<br>  
  32 GB DDR3-1600 ECC ($292)<br>  
  SC111LT-330CB 1U chassis ($200)<br>  
  Samsung 830 512GB SSD &times;2 ($1080)<br>  
  1U Heatsink ($25)<br>  
  </td>  
  <td valign="top">  
  i7-6700k Skylake 4.0 Ghz / 4.2 Ghz quad-core ($370)<br>  
  SuperMicro X11SSZ-QF-O mobo ($230)<br>  
  64 GB DDR4-2133 ($520)<br>  
  CSE-111LT-330CB 1U chassis ($215)<br>  
  Samsung 850 Pro 1TB SSD &times;2 ($886)<br>  
  1U Heatsink ($20)<br>  
  </td>  
  </tr>  
  <tr>  
  <td>$2,427</td>  
  <td>$2,241</td>  
  </tr>  
  <tr>  
  <td>  
  31w idle, 87w BurnP6 load  
  </td>  
  <td>  
  14w idle, 81w BurnP6 load  
  </td>  
  </tr>  
  </table>

  <p>So, about 10% cheaper than what we spent in 2013, with 2&times; the memory, 2&times; the storage (probably 50-100% faster too), and at least ~33% faster CPU. With lower power draw, to boot! Pretty good. Pretty, pretty, pretty, <em>pretty</em> good.</p>

  <p>(Note that the memory bump is only possible thanks to Intel finally relaxing their iron fist of maximum allowed RAM at the low end; that's new to the Skylake generation.)</p>

  <p>One thing is conspicuously missing in our 2016 build: Xeons, and <strong>ECC Ram</strong>. In my defense, this isn't intentional &ndash; we wanted the fastest per-thread performance and no Intel Xeon, either currently available or announced, goes to 4.0 GHz with Skylake. Paying half the price for a CPU with better per-thread performance than any Xeon, well, I'm not going to kid you, that's kind of a nice perk too.</p>

  <p>So <a href="https://en.wikipedia.org/wiki/ECC_memory">what is ECC all about</a>?</p>

  <blockquote>
  <p>Error-correcting code memory (ECC memory) is a type of computer data storage that can detect and correct the most common kinds of internal data corruption. ECC memory is used in most computers where data corruption cannot be tolerated under any circumstances, such as for scientific or financial computing.</p>
  
  <p>Typically, ECC memory maintains a memory system immune to single-bit errors: the data that is read from each word is always the same as the data that had been written to it, even if one or more bits actually stored have been flipped to the wrong state. Most non-ECC memory cannot detect errors although some non-ECC memory with parity support allows detection but not correction.</p>
  </blockquote>

  <p>It's <strong>received wisdom in the sysadmin community that you <em>always</em> build servers with ECC RAM</strong> because, well, you build servers to be reliable, right? Why would anyone intentionally build a server that isn't reliable? <em>Are you crazy, man?</em> Well, looking at that cobbled together Google 1999 server rack, which also utterly lacked any form of ECC RAM, I'm inclined to think that reliability measured by "lots of redundant boxes" is more worthwhile and easier to achieve than the platonic ideal of making every individual server bulletproof.</p>

  <p>Being the type of guy who likes to question stuff&hellip; I began to question. Why is it that ECC is so essential anyway? If ECC was so important, so critical to the reliable function of computers, why isn't it built in to every desktop, laptop, and smartphone in the world by now? Why is it optional? This smells awfully&hellip; <em>enterprisey</em> to me.</p>

  <p>Now, before everyone stops reading and I get permanently branded as "that crazy guy who hates ECC", I think ECC RAM is fine:</p>

  <ul>
  <li>The cost difference between ECC and not-ECC is minimal these days.</li>
  <li>The performance difference between ECC and not-ECC is minimal these days.</li>
  <li>Even if ECC only protects you from rare 1% hardware error cases that you may never hit until you literally build hundreds or thousands of servers, it's cheap insurance.</li>
  </ul>

  <p>I am not anti-insurance, nor am I anti-ECC. But I do seriously question whether ECC is as operationally critical as we have been led to believe, and I think the data shows modern, non-ECC RAM is already extremely reliable.</p>

  <p>First, let's look at the <a href="https://www.pugetsystems.com/labs/articles/Most-Reliable-Hardware-of-2014-616/">Puget Systems reliability stats</a>. These guys build lots of commodity x86 gamer PCs, burn them in, and ship them. They helpfully track statistics on how many parts fail either from burn-in or later in customer use. Go ahead and read through the stats.</p>

  <blockquote>
  <p>For the last two years, CPU reliability has dramatically improved. What is interesting is that this lines up with the launch of the Intel Haswell CPUs which was when the CPU voltage regulation was moved from the motherboard to the CPU itself. At the time we theorized that this should raise CPU failure rates (since there are more components on the CPU to break) but the data shows that it has actually increased reliability instead.</p>
  
  <p>Even though DDR4 is very new, reliability so far has been excellent. Where DDR3 desktop RAM had an overall failure rate in 2014 of ~0.6%, DDR4 desktop RAM had absolutely no failures.</p>
  
  <p>SSD reliability has dramatically improved recently. This year Samsung and Intel SSDs only had a 0.2% overall failure rate compared to 0.8% in 2013.</p>
  </blockquote>

  <p>Modern commodity computer parts from reputable vendors are amazingly reliable. And their trends show from 2012 onward essential PC parts have gotten <em>more</em> reliable, not less. (I can also vouch for the improvement in SSD reliability as we have had zero server SSD failures in 3 years across our 12 servers with 24+ drives, whereas in 2011 I was writing about <a href="http://blog.codinghorror.com/the-hot-crazy-solid-state-drive-scale/">the Hot/Crazy SSD Scale</a>.) And doesn't this make sense from a financial standpoint? How does it benefit you as a company to ship <em>unreliable</em> parts? That's money right out of your pocket and the reseller's pocket, plus time spent dealing with returns.</p>

  <p>We had a, uh, "spirited" discussion about this internally on our private Discourse instance.</p>

  <p><img src="https://blog.codinghorror.com/content/images/2015/11/discourse-2016-server-discussion.png" alt=""></p>

  <p>This is <a href="http://blog.brianmoses.net/2014/03/why-i-chose-non-ecc-ram-for-my-freenas.html">not a new debate</a> by any means, but I was frustrated by the lack of data out there. In particular, I'm really questioning <a href="https://storagemojo.com/2012/10/23/dram-errors-soft-and-hard/">the difference between "soft" and "hard" memory errors</a>:</p>

  <blockquote>
  <p>But what is the nature of those errors? Are they soft errors – as is commonly believed – where a stray Alpha particle flips a bit? Or are they hard errors, where a bit gets stuck?</p>
  </blockquote>

  <p><strong>I absolutely believe that hard errors are reasonably common.</strong> RAM DIMMS can have bugs, or the chips on the DIMM can fail, or there's a design flaw in circuitry on the DIMM that only manifests in certain corner cases or under extreme loads. I've seen it plenty. But <a href="https://en.wikipedia.org/wiki/Soft_error">a soft error</a> where a bit of memory randomly flips?</p>

  <blockquote>
  <p>There are two types of soft errors, chip-level soft error and system-level soft error. Chip-level soft errors occur when the radioactive atoms in the chip's material decay and release alpha particles into the chip. Because an alpha particle contains a positive charge and kinetic energy, the particle can hit a memory cell and cause the cell to change state to a different value. The atomic reaction is so tiny that it does not damage the actual structure of the chip.</p>
  </blockquote>

  <p>Outside of airplanes and spacecraft, I have a difficult time believing that soft errors happen with any frequency, otherwise most of the computing devices on the planet would be crashing left and right. I deeply distrust the anecdotal voodoo behind "but one of your computer's memory bits could flip, you'd never know, and corrupted data would be written!" It'd be one thing if we observed this regularly, but I've been unhealthily obsessed with computers since birth and I have never found random memory corruption to be a real, actual problem on any computers I have either owned or had access to.</p>

  <p>But who gives a damn what I think. <em>What does the data say?</em></p>

  <p>A <a href="http://www.ece.rochester.edu/~xinli/usenix07/">2007 study</a> found that the observed soft error rate in live servers was <em>two orders of magnitude</em> lower than previously predicted:</p>

  <blockquote>
  <p>Our preliminary result suggests that <strong>the memory soft error rate in two real production systems (a rack-mounted server environment and a desktop PC environment) is much lower than what the previous studies concluded.</strong> Particularly in the server environment, with high probability, the soft error rate is at least two orders of magnitude lower than those reported previously. We discuss several potential causes for this result.</p>
  </blockquote>

  <p>A <a href="http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf">2009 study on Google's server farm</a> notes that soft errors were difficult to find:</p>

  <blockquote>
  <p>We provide <strong>strong evidence that memory errors are dominated by hard errors, rather than soft errors</strong>, which previous work suspects to be the dominant error mode.</p>
  </blockquote>

  <p>Yet another <a href="http://selse.org//images/selse_2012/Papers/selse2012_submission_4.pdf">large scale study from 2012</a> discovered that RAM errors were dominated by permanent failure modes typical of hard errors:</p>

  <blockquote>
  <p>Our study has several main findings. First, we find that approximately <strong>70% of DRAM faults are recurring (e.g., permanent) faults, while only 30% are transient faults.</strong> Second, we find that large multi-bit faults, such as faults that affects an entire row, column, or bank, constitute over 40% of all DRAM faults. Third, we find that almost 5% of DRAM failures affect board-level circuitry such as data (DQ) or strobe (DQS) wires. Finally, we find that chipkill functionality reduced the system failure rate from DRAM faults by 36x.</p>
  </blockquote>

  <p>In the end, we decided the non-ECC RAM risk was acceptable for every tier of service except our databases. Which is kind of a bummer since <a href="http://www.itworld.com/article/2985214/hardware/intels-xeon-roadmap-for-2016-leaks.html">higher end Skylake Xeons got pushed back to the big Purley platform upgrade in 2017</a>. Regardless, we burn in every server we build with a complete run of memtestx86 and overnight prime95/mprime, and you should too. There's one whirring away through endless memory tests right behind me as I write this.</p>

  <p>I find it very, very suspicious that ECC &ndash; if it is so critical to preventing these random, memory corrupting bit flips &ndash; <strong>has not already been built into every type of RAM that we ship in the ubiquitous computing devices all around the world as a cost of doing business.</strong> But I am by no means opposed to paying a small insurance premium for server farms, either. You'll have to look at the data and decide for yourself. Mostly I wanted to collect all this information in one place so people who are also evaluating the cost/benefit of ECC RAM for themselves can read the studies and decide what they want to do.</p>

  <p>Please feel free to leave comments if you have other studies to cite, or significant measured data to share.</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] At Stack Overflow, we put developers first. We already help you find answers to your tough coding questions; now let us help you <a href="http://careers.stackoverflow.com" rel="nofollow">find your next job</a>.
  </td></tr>  
  </table>]]></content:encoded>
</item>
<item>
  <title><![CDATA[Building a PC, Part VIII: Iterating]]></title>
  <description><![CDATA[<p>The last time I seriously upgraded my PC was in 2011, because <a href="http://blog.codinghorror.com/the-pc-is-over/">the PC is over</a>. And in some ways, it truly is &ndash; they can slap a ton more CPU cores on a die, for sure, but the overall single core performance increase from a 2011 high end Intel</p>]]></description>
  <link>https://blog.codinghorror.com/building-a-pc-part-viii-iterating/</link>
  <guid isPermaLink="false">0b7d8df0-9757-4662-adab-983144c7d549</guid>
  <dc:creator><![CDATA[Jeff Atwood]]></dc:creator>
  <pubDate>Thu, 17 Sep 2015 21:55:00 GMT</pubDate>
  <content:encoded><![CDATA[<p>The last time I seriously upgraded my PC was in 2011, because <a href="http://blog.codinghorror.com/the-pc-is-over/">the PC is over</a>. And in some ways, it truly is &ndash; they can slap a ton more CPU cores on a die, for sure, but the overall single core performance increase from a 2011 high end Intel CPU to today's high end Intel CPU is &hellip; really quite modest, on the order of maybe 30% to 40%.</p>

  <p>In that same timespan, mobile and tablet CPU performance has continued to just about double every year. Which means the forthcoming iPhone 6s <a href="http://www.techtimes.com/articles/77083/20150818/alleged-iphone-6s-geekbench-3-results-show-2gb-ram-and-tri-core-1-5-ghz-cpu.htm">will be</a> almost <strong>10 times faster</strong> than the iPhone 4 was.</p>

  <p><a href="https://browser.primatelabs.com/ios-benchmarks"><img src="https://blog.codinghorror.com/content/images/2015/09/geekbench-single-core-iphone-results.png" alt="iPhone single core geekbench results" title=""></a></p>

  <p>Remember, that's only single core CPU performance &ndash; I'm not even factoring in the move from single, to dual, to triple core as well as generally faster memory and storage. This stuff is old hat on desktop, where we've had mainstream dual cores for a decade now, but they are <em>huge</em> improvements for mobile.</p>

  <p>When your mobile devices get 10 times faster in the span of four years, it's hard to muster much enthusiasm for a modest 1.3 &times; or 1.4 &times; iterative improvement in your PC's performance over the same time.</p>

  <p>I've been slogging away at this for a while; my current PC build series spans 7 years:</p>

  <ul>
  <li><a href="http://blog.codinghorror.com/building-a-pc-part-vii-rebooting/">Building a PC, Part VII: Rebooting</a></li>
  <li><a href="http://www.codinghorror.com/blog/2009/12/building-a-pc-part-vi-rebuilding.html">Building a PC, Part VI: Rebuilding</a></li>
  <li><a href="http://www.codinghorror.com/blog/2008/04/building-a-pc-part-v-upgrading.html">Building a PC, Part V: Upgrading</a></li>
  <li><a href="http://www.codinghorror.com/blog/2007/07/building-a-pc-part-iv-now-its-your-turn.html">Building a PC, Part IV: Now It&#39;s Your Turn</a></li>
  <li><a href="http://www.codinghorror.com/blog/2007/07/building-a-pc-part-iii-overclocking.html">Building a PC, Part III: Overclocking</a></li>
  <li><a href="http://www.codinghorror.com/blog/2007/07/building-a-pc-part-ii.html">Building a PC, Part II: Burn in</a></li>
  <li><a href="http://www.codinghorror.com/blog/2007/07/building-a-pc-part-i.html">Building a PC, Part I: Minimal boot</a></li>
  </ul>

  <p>The fun part of building a PC is that it's relatively easy to swap out the guts when something compelling comes along. CPU performance improvements may be modest these days, but there are still bright spots where performance is increasing more dramatically. Mainly in graphics hardware and, in this case, <strong>storage</strong>.</p>

  <p>The current latest-and-greatest Intel CPU is Skylake. Like Sandy Bridge in 2011, which brought us much faster 6 Gbps SSD-friendly drive connectors (although only two of them), the Skylake platform brings us another key storage improvement &ndash; the ability to connect hard drives directly to the PCI Express lanes. Which looks like this:</p>

  <p><a href="http://techreport.com/review/28446/samsung-sm951-pcie-ssd-reviewed"><img src="https://blog.codinghorror.com/content/images/2015/09/m2-drive-on-mobo.jpg" alt="" title=""></a></p>

  <p>&hellip; and performs like this:</p>

  <p><a href="http://arstechnica.com/gadgets/2015/08/intel-skylake-core-i7-6700k-reviewed/"><img src="https://blog.codinghorror.com/content/images/2015/09/Ars-Technica-Skylake-Review-Charts-012.png" alt="" title=""></a></p>

  <p><strong>Now <em>there's</em> the 3&times; performance increase we've been itching for!</strong> To be fair, a raw increase of 3&times; in drive performance doesn't necessarily equate to a computer that boots in one third the time. But here's why disk speed matters:</p>

  <blockquote>
  <p>If the CPU registers are how long it takes you to fetch data from your brain, then <a href="http://blog.codinghorror.com/the-infinite-space-between-words/">going to disk is the equivalent of fetching data from Pluto</a>.</p>
  </blockquote>

  <p>What I've always loved about SSDs is that they attack the <strong>PC's worst-case performance scenario</strong>, when information has to come off the slowest device inside your computer &ndash; the hard drive. SSDs massively reduced the variability of requests for data. Let's compare L1 cache access time to minimum disk access time:</p>

  <blockquote>
  <p>Traditional hard drive <br>
  0.9 ns &rarr; 10 ms (variability of 11,111,111× )</p>
  
  <p>SSD <br>
  0.9 ns &rarr; 150 µs (variability of 166,667× )</p>
  </blockquote>

  <p>SSDs provide a reduction in overall performance variability of 66×! And when comparing latency:</p>

  <blockquote>
  <p><a href="http://storagereview.com/toshiba_sata_hdd_enterprise_35_review_mg03acax00">7200rpm HDD</a> &mdash;  1800ms <br>
  <a href="http://storagereview.com/intel_ssd_dc_s3500_enterprise_review">SATA SSD</a> &mdash; 4ms <br>
  <a href="http://storagereview.com/huawei_tecal_es3000_application_accelerator_review">PCIe SSD</a> &mdash; 0.34ms  </p>
  </blockquote>

  <p>Even going from a fast SATA SSD to a PCI Express SSD, you're looking at a 10x reduction in drive latency.</p>

  <p>Here's what you need:</p>

  <ul>
  <li><a href="http://www.amazon.com/dp/B01639696U/?tag=codihorr-20">256GB Samsung 950 Pro NVMe M.2 drive</a> $198</li>
  <li><a href="http://www.amazon.com/dp/B012NH05UW/?tag=codihorr-20">Asus Z170-A motherboard</a> $165</li>
  <li><a href="http://www.amazon.com/dp/B012M8M7TY/?tag=codihorr-20">Intel i5-i6600k Skylake CPU</a> $270</li>
  <li><a href="http://www.amazon.com/dp/B00TPQPOIS/?tag=codihorr-20">16GB DDR4 memory</a> $134</li>
  </ul>

  <p>These are the basics. It's best to use the M.2 connection as a fast boot / system drive, so I scaled it back to the smaller 256 GB version. I also had a lot of trouble getting my hands on the faster i7-6700k CPU, which appears supply constrained and is currently overpriced as a result.</p>

  <p>(Also, be careful, as some older M.2 drives can use the older <strong>AHCI</strong> connection type. Make sure yours is <strong>NVMe</strong>, as <a href="http://www.anandtech.com/show/7843/testing-sata-express-with-asus/4">the performance difference can be substantial</a>.)</p>

  <p>Even though the days of doubling (or even 1.5&times;-ing) CPU performance are long gone for PCs, there are still some key iterative performance milestones to hit. Like <a href="http://blog.codinghorror.com/our-brave-new-world-of-4k-displays/">mainstream 4k displays</a>, I believe mainstream PCI express SSDs are another important step in the overall evolution of desktop computing. Or <a href="http://blog.codinghorror.com/the-pc-is-over/">its corpse</a>, anyway.</p>

  <table>  
  <tr><td class="welovecodinghorror">  
  [advertisement] <a href="http://careers.stackoverflow.com" rel="nofollow">Find a better job the Stack Overflow way</a> - what you need when you need it, no spam, and no scams.
  </td></tr>  
  </table>]]></content:encoded>
</item>
</channel>
</rss>
